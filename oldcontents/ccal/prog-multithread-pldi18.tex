
\section{Building Certified Multithreaded Layers}
\label{sec:multithreaded-layers}
Multithreaded programs have to deal with interleavings triggered by not only  the hardware scheduler
but also the  explicit invocation
of thread scheduling primitives. 
Therefore, introducing multithreaded layer interface requires additional works to handle thread scheduling primitives. 
First, our framework has to abstract away other threads' behavior 
as an environmental context associated with the thread.
This work is similar with the corresponding parts of our multiprocessor 
layer interface in a sense that abstracting away other components' behavior as environmental contexts.
Secondly, we want to achieve the goal with using the existing framework as much as possible, especially 
the compiler, CompCertX, that we have used in the previous work~\cite{dscal15}.
Even though our framework can handle those issues, showing that a 
large concurrent program can be verified and linked in our framework is an another work. 
Our CCAL Toolkit resolve all the issues.
In this section, we introduce  the certified layers
dealing with scheduling primitives, a new concept of thread-local layer interfaces equipped with
compositional rules, and a thread-safe version of CompCertX.

\subsection{Multithreaded Layer Interface Overview}\label{subsec:machinemodeloverview}

The purpose of multithreaded layer interface is expose the framework that makes users to be able to
program, compile, verify multithreaded programs on top of it with keeping the 
consistency with CPU-local layers that we have built.
In this sense, 
one requirement that the interface has to achieve is an ability to 
link multiple machine models (\textit{i.e.} 
we call them $LAsm_{H}$) in thread-local layers 
with one single machine (\textit{i.e.} we call it $LAsm_{L}$)
model in CPU-local layer while 
preserving the common framework in both sides (multiprocessor layer and multithreaded layer interfaces) 
as much as possible. 
We especially want to re-use the compilation phases from C programs to Assembly programs (CompCertX~\cite{dscal15}) 
in both side due to the size and complexity of those parts. 
However, providing the common framework in both sides 
cannot be done easily due to multiple challenges that we have described. 
To bridge the gap between the  machine model in CPU-local layers and
that in thread-local layers,
we introduce two intermediate machine models, $EAsm$ and $TAsm$. 

\begin{figure}[t]
\includegraphics[scale=.40]{figs/ccal/thread-linking}
\caption{Thread Linking}
\label{fig:thread-linking}
\end{figure}

Figure~\ref{fig:thread-linking} briefly shows how those intermediate machine models help us to 
resolve multiple challenges.
The multiprocessor layer interface (c.f\ Fig.~\ref{fig:thread-linking} (1)) is still a CPU-local layer,
and the layer contains only one register set and one private abstract data in its state which are associated
with the CPU.
The layer definitely captures the execution of the whole thread set of CPU $c$ 
and does not support thread-local reasoning.
The first step of building multithreaded layer interface is 
dividing CPU-local private data (a private register set and a private abstract data) into multiple thread-local
private datum (c.f\ Fig.~\ref{fig:thread-linking} (2)). 
Since the layer contains multiple private datum, we also add the flag for currently-running thread ID $curid$ in the state. 
At this stage, 
context switching becomes simple because we do not need to store and load the corresponding contexts from/to 
the abstract data structure to/from the registers. 
Instead of that, the machine only needs to change the current thread id ($curid$) to mark 
the thread id that currently has the running control. 

Now each thread can use its own private data for its evaluation, but that is not sufficient at all. 
In fact, scheduling switches in this layer (c.f\ Fig.~\ref{fig:thread-linking} (2)) 
has a similar meaning with the ones in the lower layer,
which implies that changing the context from one to an another one for the evaluation.
Ideally, we would like to reason about each thread execution 
independently, and later formally combine the reasoning to obtain a global
property for the full set of threads on the same CPU.
So, we need a machine model that gives semantics to
a partially-composed set of threads to support this.

Therefore,  a new layer (c.f\ Fig.~\ref{fig:thread-linking} (3)) is introduced such that other 
threads' operations can be modeled as input strategies to the layer interface 
(as we did in the multiprocessor layer interface). 
Here, we introduced a new kind of environment context, $\oracle^{t}$.
This $\oracle^t$ will only be queried during the execution of scheduling primitives (thread yield/sleep/wakeup),
since our machine model does not allow
preemption. 
Similar to the environmental context query in the multiprocessor layer interface, 
The execution has two kinds of
behaviors  depending on whether the \emph{target
thread} is active or not.
Considering an execution in Fig.~\ref{fig:thread-linking} (3) with an active thread set
$T = \{0\}$, whenever an execution switches (by $\yield$ or $\sleep$) 
to a thread outside of $T_a$ (i.e., the yellow $\yield$),
it takes environmental steps (i.e., notated as arrows), repeatedly appending the 
events returned by the environment context $\oracle$ and the thread
context $\oracle^t$ to the log until a $\yield$
event indicates that control switches back to an active thread.

This layer is already a thread local because it only captures the behavior of one thread.
However, the strategy query in this layer follows small-step style, 
and this is insufficient to build thread-local layer interface because 
we do not want to query multiple times for a single yield or sleep calls.
Therefore, we introduce another layer (we call the machine model in this layer $TAsm$)
to merge those multiple strategy queries into a single 
big-query (c.f\ Fig.~\ref{fig:thread-linking} (4)). 
Finally, the last thing to do to build a multithreaded layer interface
is to connect the machine state of the current intermediate machine ($TAsm$) 
to the similar form of 
our multiprocessor
 layer interface, which has the form of $(\regs, m, a, l)$.
Therefore, we introduce the last layer (Fig.~\ref{fig:thread-linking} (5)) that will become a 
base in building multithreaded layers.

During the remaining parts of this section, we will first explain  all those machine models 
that are introduced in Fig.~\ref{fig:thread-linking}.
We also show key ideas to prove the compositional theorems to link these machine models. 
After that, we explain how we build and refine the concrete layer interface over them. 
As for the second part, we also show the refinement relation between concrete layer definitions.
As the last part of this section, we talk about one another subtlety issue about memory to build multithreaded layer interface. 


\subsection{Certified Layers for Scheduling Primitives}\label{subsec:pbthreadlayer}
Based on the shared thread queues provided by the multicore
toolkit (c.f\ Sec.~\ref{sec:shared-queue}), we introduce a new  layer interface
$\Lbthread[c]$ that supports multithreading.
At this layer interface, the transitions between threads are done using
scheduling primitives,
implemented in a mix of
C and assembly.

Figure~\ref{fig:exp:sched} shows the implementation
of scheduling primitives using CCAL.
\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/ccal/scheduling.c}
\caption{Pseudocode of selected scheduling primitives.}
\label{fig:exp:sched}
\end{figure}


In our multithreaded setting, each CPU $c$ has a private ready queue $\commc{rdq}$ 
and a shared pending queue $\commc{pendq}$ (containing the threads woken up by other CPUs). 
A thread yield sends the first pending thread from
 $\commc{pendq}$ to $\commc{rdq}$ and then
switches to the next ready thread. 
There are also many shared sleeping queues
$\commc{slpq}$. 
When a sleeping thread is woken up,
it will be directly appended
to the ready queue if the thread belongs to the currently-running CPU. 
Otherwise, it
will be appended to the pending queue of the CPU it belongs to.

Thread switching is implemented by the context switch function $\commc{cswitch}$, which
saves the current thread's kernel context (i.e., $\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}$),
and loads the context of the target thread.
This  $\commc{cswitch}$ (invoked by $\commc{yield}$ and $\commc{sleep}$) can only be implemented at the assembly level,
as it does not satisfy the C calling convention.
A scheduling primitive like $\commc{yield}$  first queries  $\oracle$ to update the log,
appends its own  event, and then invokes $\commc{cswitch}$ to transfer the control.%
\[
\includegraphics[width=.5\textwidth]{figs/ccal/thread1}%
\]%
\noindent{}This layer interface introduces three new events
$c.\yield$, $c.\sleep(i, lk)$
(sleep on queue $i$ while holding the lock $lk$), and $c.\wakeup(i)$ (wakeup the queue $i$).
These events record the thread switches, which can be used to track the currently-running thread
by a replay function $\replay_{\comm{sched}}$.
These three events are converted from the events
of multicore toolkit, i.e.\,
$c.\deq(\comm{pendq}(c))$,
$c.\enq(\comm{slpq}(i))$
and $c.\enq(\comm{pendq}(c))$, respectively.
These events encapsulate
the modifications to the 
shared thread control blocks ($\comm{tcbp}$), 
thread queues ($\comm{tdqp}$), 
and the running thread \allid{} ($\comm{tid}$).
Thus, the corresponding abstract states
are hidden at $\Lbthread$ but
can always be reconstructed
by the replay function $\replay_{\comm{sched}}$ given the current log.

%%% FULL EASM %%%%%%%%
\subsection{Intermediate Concurrent Machine Model}\label{subsec:fulleasm}

The CPU-local interface  $\Lbthread[c]$
captures the execution of the whole
thread set of CPU $c$ and does not support thread-local verification.
Ideally, we would like to formally reason about each thread separately
 and later compose the proofs together to obtain a global
property.
Thus, we introduce a new layer interface and a new intermediate concurrent 
 that is compositional 
and only focuses on a subset of threads of CPU $c$.
To support this, we need a machine model ($EAsm$) that gives semantics to
a partially-composed set of threads.

Let $T_c$ denote the whole thread set running over CPU $c$.
Based upon  $\PLayer{L}{c}{}$,  we construct a 
\emph{multithreaded} layer interface $\TLayer{L}{c}{T_c}$ without introducing 
additional rely/guarantee conditions.
The purpose of this step is dividing the  CPU-local state as multiple thread-local states
(from Fig.~\ref{fig:thread-linking} (1) to  Fig.~\ref{fig:thread-linking} (2)) as follows:
\[(\textit{State})\ \ \ s_{EAsm}  :=  (tid, f_{\regs}, m, f_{a},l)\]
, when $f_{\regs_{T}}$ is a partial amp from thread id to private state (registers), $f_{a}$ is a partial
map from thread id to divided abstract states based on the abstract state ($a$) in CPU local layer $\Lbthread[c]$.
This change implies that all threads will have their designated registers and local states.
One another purpose of introducing this layer is replacing scheduling primitives in $\Lbthread[c]$ layer 
with explicit transition rules in its machine model to
replace assembly style context switch ($\commc{cswitch}$) into a no-op 
like operation. 
To do that, we have added software scheduler related rules ($\comm{yield}$ and $\comm{sleep}$) 
in this intermediate machine model ($EAsm$). 
For example, the execution of $\commc{yield}$ in~Sec.\ref{subsec:pbthreadlayer} will be replaced by
the transition rule:
\begin{center}
\begin{tikzpicture}[->,>={stealth[black]}, auto,  node distance=3cm,draw]
\begin{scope}[every node/.style={font=\sffamily\small}]
    \node (A) at (0,0) {};
    \node (B) [node_w] at (0.13,0) {};
    \node (C) [node_db] at (6.13,0) {};
\end{scope}

\begin{scope}[every node/.style={font=\sffamily\footnotesize},
every edge/.style={draw, thick}]
    \path [->] (A) edge (B);
    \path [->] (B) edge node[above]{(exec$\_$yield$_{EAsm}$)} node[below]{$(tid, f_{\regs}, m, f_{a},l) \rightarrow (tid', f_{\regs}, m, f_{a},l')$} (C);
\end{scope}
\end{tikzpicture}
\end{center} 
when $l'$ is ``$l \cdot tid.\commc{yield} (tid \switch tid')$''.
This implies that the $\commc{yield}$ evaluation will only switch the current running thread and update the current global log 
by adding the corresponding event to the log like we the hardware scheduler do in our concurrent multicore machine model.
Thanks to having these explicit evaluation rules for scheduling primitives in Sec.~\ref{subsec:pbthreadlayer} as well as
hread-local registers and private data structures associated with each thread, 
context switching will become much simpler in this level than the assembly style context switching 
in the previous level that we have described in Sec.~\ref{subsec:pbthreadlayer}.


subsection{Intermediate Concurrent Machine Model and Thread Linking}\label{sec:multi-threaded-partial}
The next step in defining thread-local layer interface
is replacing other threads' evaluation using the strategy as we have already seen in Fig.~\ref{fig:thread-linking}. 
In this step, our machine does not guarantee that the scheduled thread id is always a member 
running or available threads because the machine is not a total machine on CPU $c$. 
If the thread is not in both cases, we categorize it as a thread with an $\mathrm{Environment}$ state.
Formally, let's  assume the set $T_a$ which satisfies $T_a \subseteq T_c$.
Based upon  $\PLayer{L}{c}{}$,  we construct a 
 \emph{multithreaded} layer interface $\TLayer{L}{c}{T_a} := (\PLayer{L}{c}{}.\Layer,
 L[c].\Rely\cup \Rely_{T_a}, L[c].\Guard_{|T_a})$,
which is 
parameterized over a focused thread set $T_a \subseteq T_c$.
Besides $T_q$, strategies of other threads running on $c$ form a thread context $\oracle^{t}$.
Rely conditions of this multithreaded layer interface extend $L[c].\Rely$ with a \emph{valid set} of $\oracle^{t}$ (denoted as ``$\Rely_{T_a}$'') and
guarantee conditions replace $L[c].\Guard(c)$ with the invariants held by threads in $T_a$ (denoted as ``$L[c].\Guard_{|T_a}$''). Since our machine model does not allow
preemption, $\oracle^{t}$ will only be queried during the execution of scheduling primitives, which have two kinds
of behaviors  depending on whether the \emph{target
thread} is focused or not.%
\[
\includegraphics[width=.5\textwidth]{figs/ccal/thread2}
\]%
\noindent{}Consider the above execution with 
$T_a = \{0,1\}$. Whenever an execution switches (by $\commc{yield}$ or $\commc{sleep}$) 
to a thread outside of $T_a$ (i.e., the yellow $\commc{yield}$ above),
it takes environmental steps (i.e., inside the red box), repeatedly appending the 
events returned by the environment context $\oracle$ and the thread
context $\oracle^t$ to the log until a $c.\yield$
event indicates that the control has switched back to a focused thread.
Whenever an execution  switches to a focused one (i.e., the blue $\commc{yield}$ above), 
it will  perform the context switch without asking $\oracle/\oracle^t$
and its behavior  is identical to the one of $\Lbthread[c]$.

\para{Composing Multithreaded Layers.}
Multithreaded layer interfaces with disjoint focused thread sets
can also be composed in parallel (using an extended \textsc{Pcomp} rule) if the guarantee condition implies
the rely condition for every thread.
If the active thread sets $T_{A1}$ and $T_{A2}$ of $\TLayer{L}{c}{T_{A1}}$ and $\TLayer{L}{c}{T_{A2}}$  machines
are disjoint, they can be composed together 
to form a machine with the union active thread set $T_{A1} \bigcup T_{A2}$.
The resulting focused thread set is the union of the composed ones,
and some environmental steps
are ``replaced by'' the local steps of the other thread set.
For example, if we compose $T_a$ in the above example
with thread 2, the previously yellow $\commc{yield}$ of thread 0 will then switch to
a focused thread.
\[
\includegraphics[width=.5\textwidth]{figs/ccal/thread3}
\]%
Here, the event list $l_1$ generated by $\oracle$ and $\oracle^t$
has been divided into two parts: ``$l_{1a}\cons c.\yield$'' (generated by thread 2)
and $l_{1b}$ (consisting of events from threads 
outside \{0,1,2\}).

Thanks to this parallel composition rule, given each multithreaded layer $\TLayer{L}{c}{t}$
with a single active thread $t\in T_c$,
we can repeatedly compose them together
and build a layer for the entire thread set on $c$.

\para{Multithreaded Linking.}
When the whole $T_c$  is focused,
all scheduling primitives fall into the second case and never switch to unfocused ones. Thus, 
its scheduling behaviors are equal to the ones of $\Lbthread[c]$. 
To link multiple thread-local machines as one multithreaded concurrent machine,  
we perform the above iteration of building a single-threaded concurrent machine model in a reversed way.
Let us first focus on two single-threaded concurrent machine models, 
$\EAsmM{[c, \{0\}]}$ and $\EAsmM{[c, \{1\}]}$.
Then, partial maps for thread private data ($f_a$) of $\EAsmM{[c, \{0\}]}$ 
and of $\EAsmM{[c, \{1\}]}$ will be defined as\newline
\noindent
\begin{minipage}[t]{.5\textwidth}
\begin{small}
\[
f_{a}(i):=
\begin{cases}
 \comm{Some \ a} & \comm{when} \ i = 0\\
\comm{None} & \text{otherwise}
\end{cases}
\]
\end{small}
\end{minipage}
\begin{minipage}[t]{.5\textwidth}
\begin{small}
\[
f_{a}(i):=
\begin{cases}
 \comm{Some \ a} & \comm{when} \ i = 1\\
\comm{None} & \text{otherwise}
\end{cases}
\]
\end{small}
\end{minipage}
respectively.
Then, merging those two partial maps will be 
\begin{small}
\[
f_{a}(i):=
\begin{cases}
 \comm{Some \ a} & \comm{when} \ i = 0 \vee i = 1\\
\comm{None} & \text{otherwise}
\end{cases}
\]
\end{small}
, which simply merge two maps together. 
For the partial map of the private register set ($f_\regs$), merging
is using the corresponding register set with the current running thread $tid$.
In terms of environmental context, both machines have the same environmental context, which is 
$\oracle^{T}$ even though they have different strategies from each other ($\oracle^{T}[c, 0])$ and $\oracle^{T}[c, 1]$).
With those environmental contexts, 
building environmental context for the thread set $\{0, 1\}$ is straightforward. 
We need to either  exclude the event generated by thread 1 from $\oracle^{T}[c, 0]$ or 
exclude the event generated by thread 0 from $\oracle^{T}[c, 1]$.
Then, having those two single-threaded local machines,
we can merge their private states, registers, and a environmental context, 
and thus build a two-threaded concurrent machine model, $EAsm_{[c, \{0, 1\}]}$.
If $T_c = \{0, 1\}$, then we do not need further merging, 
and the machine directly turns into a total concurrent machine 
model $EAsm_{[c, T_c]}$. 
If not, we pick one thread $i$ (\textit{i.e.} $i \in (T_c - \{0, 1\})$), 
and merge the thread again with the exactly 
same process that we have done for thread $0$ and thread $1$.
Finally,
we can prove
the following theorem:
\begin{theorem}[Multithreaded Linking]
\label{thread_composition}
$$
\PLayer{\Lbthread}{c}{\oracle}\le_\id \TLayer{\Lhthread}{c}{T_c}
$$%
\end{theorem}%
\noindent This theorem guarantees that,
once the multithreaded machine based on $\Lhthread[c][T_c]$ captures
the whole thread set,
 the properties of  threads running on top
can be propagated down to the layer with concrete
scheduling implementations.

\subsection{Intermediate Thread-Local Machine Model}\label{subsec:tasm}
If the previous machine contains only one single thread,
the machine is already a single threaded machine model, which gives us a full isolation with other threads on the same CPU.
However, $EAsm$ itself is quite different with $LAsm_L$ in terms of its state definition and evaluation rules,
so it is hard for us to show the refinement relation between this $EAsm$ and $LAsm_L$ directly. 
Especially, yield back and environmental steps in $EAsm$ does not match well with our $LAsm_L$ rules. 
If we keep those operational style strategy query evaluation, it is hard for us to define a single step behavior of 
scheduling primitives in our thread-local layer interface.
Due to the differences between $EAsm$ and $LAsm_L$, 
we have introduced one more intermediate machine model, $TAsm$,
which is parameterized by the current CPU ID ($c$) and and the thread id ($t$). 
And using these variables, we define this machine's state as
\begin{small}
\[
st = (t, \ \regs, m, a, l) 
\]
\end{small}
, which has a shared log and only one private data for the thread.
In this machine, all other threads' steps should be replaced by environmental steps
as our single-threaded concurrent machine model, $EAsm$, does.
In terms of environmental context query, however, 
this new intermediate machine model ($TAsm$) has a different style (See Fig.~\ref{fig:thread-linking} (3) and Fig.~\ref{fig:thread-linking} (4)).
The previous model ($EAsm$ with a single thread, Fig.~\ref{fig:thread-linking} (3)) 
queries its environmental context via operational style as we have marked as multiple arrows in the figure. 
On the other hand, $TAsm$ queries its environmental context as a big-step style, and 
will directly return to the current thread id with the updated log in a single evaluation. 
For example, yield evaluation rule in this intermediate machine model will have the following single step transition:
\begin{center}
\begin{tikzpicture}[->,>={stealth[black]}, auto,  node distance=3cm,draw]
\begin{scope}[every node/.style={font=\sffamily\small}]
    \node (A) at (0,0) {};
    \node (B) [node_w] at (0.13,0) {};
    \node (C) [node_db] at (6.13,0) {};
\end{scope}

\begin{scope}[every node/.style={font=\sffamily\footnotesize},
every edge/.style={draw, thick}]
    \path [->] (A) edge (B);
    \path [->] (B) edge node[above]{(exec$\_$yield$_{TAsm}$)} node[below]{$(tid, {\regs}, m, {a},l) \rightarrow (tid', {\regs}, m, {a},l')$} (C);
\end{scope}
\end{tikzpicture}
\end{center}
, where the thread id is always remained as same (\textit{i.e.} $tid = tid'$),
and $l' = l \cdot l''$ when $l'' = (?(\oracle, \oracle^{T}), !tid.\commc{yield}(tid \switch tid''))$
 and $tid'' \in T_c$.
This difference is quite small, but works as an another crucial part to build  multithreaded layer interface 
that uses $LAsm$ like machine model and the approach that we have discussed in the previous parts of this section. 

\subsection{Thread-Local Machine Model}\label{subsec:hasm}
By applying the whole processes,
we finally can define the machine model $LAsm_H$ 
which has lots of common aspects with the machine model of CPU-local layer interface ($LAsm_L$)
enough to use the existing compiler \compcertx~\cite{dscal15} as it is.
For example, the yield call will perform the transition as follows:
\begin{center}
\begin{tikzpicture}[->,>={stealth[black]}, auto,  node distance=3cm,draw]
\begin{scope}[every node/.style={font=\sffamily\small}]
    \node (A) at (0,0) {};
    \node (B) [node_w] at (0.13,0) {};
    \node (C) [node_db] at (6.13,0) {};
\end{scope}

\begin{scope}[every node/.style={font=\sffamily\footnotesize},
every edge/.style={draw, thick}]
    \path [->] (A) edge (B);
    \path [->] (B) edge node[above]{(exec$\_$yield$_{LAsm_H}$)} node[below]{$({\regs}, m, {a},l) \rightarrow ({\regs}, m, {a},l')$} (C);
\end{scope}
\end{tikzpicture}
\end{center}
, where  $l' = l \cdot l''$ when $l'' = (?(\oracle, \oracle^{T}), !tid.\commc{yield}(tid \switch tid'))$,
$tid$ is the current thread id, and $tid' \in T_c$.
Since this machine will always have the same current thread id as its player, the machine state 
does not need to keep the running thread id information any more. 
This implies that we are able to 
utilize the whole power of \compcertx\ and build thread-local layers both written in 
C and in Assembly with the guarantee that those layers preserve the same behavior in our CPU-local machine.
In addition to that, scheduling primitives ($\commc{yield}$ and $\commc{sleep}$) only updates its global log, so these primitives calls
follow C calling convention. 
In this sense we can use those primitives when we program and verify C functions in thread local layers ($\Lhthread[c]$), which was 
not possible in our CPU-local layers ($\Lbthread[c]$).
 
This machine model, however, has one big difference with $LAsm_L$ (that is why we have
distinguished $LAsm_L$ with $LAsm_H$ in their notations).
This machine need to allow a dynamic initial state for each thread, and this dynamically assigned 
initial state should be satisfied by our system invariant.
The reason of enabling dynamic initial states is that machine sates of threads on one CPU are 
closely related to  others' behaviors.
For example,
all threads have to wait until its parents spawn them at some point of the whole system's view.
While waiting the spawning, the state that corresponds to the thread in the system will be changed  
(before this arbitrary thread starts its evaluation), 
so the machine model ($LAsm_H$) of the thread needs to capture it properly while 
keeping the consistency between the state of the whole system and the state of the thread.
That is not only for the shared state (notated as our global log) 
but also for the thread private data structures, such as 
page allocation table and container information, due to the initialization of those data structures. 
In detail, the main thread of the system should initialize all threads' private datum (\textit{i.e.}
page allocation table, container information, etc)
and all other threads should use the properly initialized their private data structures 
even though they do not have a power to initialize them.
Also, the initial state of each thread should have the proper value to some of its private data before it starts its evaluation. 
For instance, the quota in the thread's container information should have the proper value that its 
parent have allowed while the parent spawn the thread.
To resolve the problem with keeping its generality,
our thread-local layer interface provides one abstract definition.

The abstract function to calculate the initial state (for each thread's private data) has a type of
\begin{small}
\[
fun_{st_{init}} : \mathbb{Z} \rightarrow l \rightarrow (\regs, a, l)
\]
\end{small}
which gets the current thread id and a initial log (mapped with the current thread id) as its arguments 
and returns a \textit{initial private register set}, \textit{initial private abstract data} and a initial log for the 
current thread id.
Using this function, 
this machine model can assign proper initial states for all threads even though 
the machine is fully isolated from other threads' machines.

This definition also gives consistency between our multithreaded concurrent machine model 
and this thread-local machine model.
When looking at the initial state definition for $EAsm$ in Sec.~\ref{subsec:fulleasm},
finding the similarity between both definitions is straightforward.
For the guarantee about preserving our system invariant in the initial state, we only need to prove that 
the calculated initial state satisfies our invariant.


\subsection{Thread-Local Layer Interface} \label{subsec:phthreadlayer}
If a multithreaded interface $\TLayer{L}{c}{t}$  focus only on
 a single thread $t\in T_c$,
 $\commc{yield}$ and $\commc{sleep}$ primitives  always
switch to an unfocused thread and then repeatedly query $\oracle$ and $\oracle^t$
until yielding back to $t$ as we have discussed in Sec.~\ref{subsec:hasm}.
\[
\includegraphics[width=.5\textwidth]{figs/ccal/thread4}
\]%
We can prove that this yielding back procedure in our system always terminates. This proof relies on the fact that the software scheduler is \emph{fair} and every running
thread gives up the CPU within a finite number of steps.
In this sense, and using the machine model that we explain in Sec.~\ref{subsec:hasm}, 
We call $\TLayer{L}{c}{t}$ a ``thread-local'' layer interface
because scheduling primitives
always  end up switching back to the same thread;
they do not modify the kernel context 
(i.e.\, $\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}$) and effectively act as a ``no-op'',
except that the shared log gets updated.
Thus, these scheduling primitives indeed satisfy C calling conventions  as you have seen in Sec.~\ref{subsec:hasm}.



\subsection{Queuing Lock}\label{subsec:qlockimplementation}

\begin{figure}[t]
\lstinputlisting [language = C, multicols=2] {source_code/ccal/queue_lock.c}
\caption{Pseudocode of queuing lock.}
\label{fig:exp:qlock}
\end{figure}
Based upon thread-local layer interfaces,
we build additional synchronization toolkits, such as
a queuing lock (c.f\ Fig.~\ref{fig:exp:qlock}).
With  queuing locks, waiting threads are put to sleep to avoid busy spinning.
Reasoning about this locking algorithm is particularly
challenging since its C implementation utilizes both
spinlocks and low-level scheduler primitives (i.e.\, $\commc{sleep}$ and $\commc{wakeup}$).
This verification task can be decomposed into a bunch of layers above $\TLayer{\Lhthread}{c}{t}$ using CCAL.

The correctness property of a queuing lock consists of
two parts: mutual exclusion and starvation freedom.
The lock implementation (Fig.~\ref{fig:exp:qlock}) is mutually exclusive
because the busy value of the lock ($\commc{ql\_busy}$)
is always equal to the lock holder's thread \allid{}.
This busy value is set either
by the lock requester when the lock is free (line 6 of 
Fig.~\ref{fig:exp:qlock})
or by the previous lock holder when releasing the lock
(line 12).
With the atomic interface of the spinlock, the starvation-freedom proof of queuing lock
is mainly about the termination of the sleep primitive call
(line 4). By showing that all the lock holders
will eventually release the lock,
we prove that all the sleeping threads will be 
added to the pending queue or ready queue within a finite number
of steps. Thus, $\commc{sleep}$ will terminate
thanks to the \emph{fair} software scheduler.
Note that all these properties proved at the C level can be propagated down to the assembly level using the thread-safe CompCertX.
