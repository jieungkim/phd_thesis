\section{Building Certified Multithreaded Layers}
\label{sec:multithreaded-layers}
We continue to illustrate the layer-building patterns, this time
considering objects shared among different threads.
We first introduce a concurrent layer interface $\Lbthread$ by verifying
scheduling primitives based on the multicore toolkit.
Then we introduce the \emph{multithreaded} layer interface, which becomes a \emph{thread-local} layer
when taking a single active thread.
This allows us to perform thread-local reasoning for each thread, whereby the proved properties of each thread can be linked formally
to obtain a global claim about the whole set of threads.
On top of this thread-local layer interface,
we verify a queuing lock implementation  in our CCAL toolkit. 
\ignore{and establish an end-to-end verified  example
using multithreading toolkit. }
\ignore{
scheduling primitives, going from implementations consisting of C and assembly programs,
to compositional specifications that have \emph{local} meanings at the C level.

To the best of our knowledge, this is the first attempt to support modular verification
of multithreaded applications with explicit synchronization using verified
low-level scheduler primitives. 
\david{Is this last sentence necessary? It has very specific qualifications and
weak wording (``first attempt to support'').}

\david{I think we need a high-level overview of the rest of the
section here. I struggled a lot with why we were going from
one subsection to the next.}}

\ignore{
\ronghui{The following paragraph is obsolete}
\paragraph{[[[Scheduler primitives and multi-thread composition}
It is common for multi-threaded programs to perform explicit synchronization
of threads by calling scheduler primitives, e.g., $\yield$, $\sleep$, and $\wakeup$.
Verification of such concurrent programs is extremely challenging.
Previous work either directly models the abstract scheduler primitives at a high level
(e.g., simple switch in the thread ID~\cite{xu16}),
or verifies the scheduler functions with a low level small-step semantics
(e.g., saving and restoring the register set) \cite{dscal15}.
Both approaches have severe limitations. The first approach provides no formal connection
between the specification and underlying C and assembly implementation of those
scheduler functions. In the second approach, the specifications are too low level to
be useful in verifying the actual programs calling these primitives. Furthermore,
some portion of scheduler functionality is implemented in assembly (e.g., context switch).
Thus the semantics does not satisfy the C calling convention, meaning that it 
is impossible to reason about programs calling these assembly functions at the C level.
In this example, we build concurrent layers for the assembly part
of a scheduler in a small-step manner, but later lift the specifications
to big-step ones satisfying the C calling conventions. ]]]}


\ifTR{}{\vspace{-5pt}}
\subsection{Concurrent Layer Interface with Scheduling Primitives}\label{subsec:pbthreadlayer}
Based on the shared thread queues provided by the multicore
toolkit (\cf Sec.~\ref{sec:shared-queue}), we introduce a new  layer interface
$\Lbthread$ that supports multithreading.
At this layer, the transitions between threads are done using
scheduling primitives,
implemented in a mix of
C and assembly.

Figure~\ref{fig:exp:sched} shows the implementation
of selected scheduling primitives.
In our multithreaded setting, each CPU $c$ has a private ready queue $\comm{rdq}$ 
and a shared pending queue $\comm{pendq}$ (containing the threads woken up by other CPUs). 
A thread yield appends the first pending thread from
 $\comm{pendq}$ to $\comm{rdq}$ and then
switches to the next ready thread. 
There are also many shared sleeping queues
$\comm{slpq}$. 
When a sleeping thread is woken up,
it will be directly appended
to the ready queue if the thread belongs to the currently-running CPU. 
Otherwise, the thread
will be appended to the pending queue of the CPU it belongs to.

Thread switching is implemented by the context switch function $\mathsf{cswitch}$, which
saves the current thread's kernel context (including the 
\emph{stack pointer}),
and loads the context of the target thread.
This  $\mathsf{cswitch}$ (invoked by $\yield$ and $\sleep$) can only be implemented at the assembly level,
as it does not satisfy the C calling convention.
\ignore{Since $\yield$ and $\sleep$ invoke this hand-written assembly function 
$\mathsf{cswitch}$, they also do not satisfy the C calling convention.}
Thus, the scheduling primitives at this layer
are defined using assembly-style specifications (denoted as $\vdash_{\comm{Asm}}$).
\ignore{
\jieung{I'm not sure whether we need this sentence or not - mentioning that that is a assembly funciton}
\david{Inconsistencies with figure: no wakeup function, pendq doesn't
take a parameter, etc.}
\david{This paragraph is really hard to follow, as I don't believe we've
explained the high-level setup for ready queue, pending queues, and
sleep queues.}
\newman{I also think that we should explain notations explicitly. We can exclude
the notations from descriptions above, and separately explain that rdq() returns
the ready queue for the current CPU, etc.}

\jieung{Similar opinion. And, When I first read the sentence about ready and pending queues, my impression was both of them are private ones because they are parameterized by $c$}}


%The specifications of $\yield$ and $\sleep$ are defined as follows:
\begin{small}
\ifTR{}{\vspace{-10px}}
\begin{mathpar}
\inferrule{
l_0 = \Query{\oracle}{l}{c} \\
\comm{tid} = \replay_{\comm{sched}} (l_0, c) \\
a' = a\set{\comm{kctxt}(\comm{tid}) : \regs[\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]} \\
l' = l_0 \cons c.\yield\ignore{/c.\sleep(i,lk)} \\
\comm{rtid} = \replay_{\comm{sched}} (l', c) \\
\regs' = \regs\leftarrow a.\comm{kctxt}(\comm{rtid})  
}{
 \oracle, c\vdash_\comm{Asm} \sstep{\spec_{\yield\ignore{/\sleep}}}{[]\ignore{/[i, lk]}}{\regs, m, a, l}{\regs', m, a', l'}
}
\end{mathpar}
%\vspace{-5px}
\end{small}%
The $\yield$ function saves and restores the 
appropriate kernel context (i.e., $\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}$) using the register 
set $\regs$ and the abstract context $a.  \comm{kctxt}$.
Then it appends a  $\yield$  event to the logical log
after querying  $\oracle$. 
\[
\includegraphics[width=.50\textwidth]{figs/thread1}
\]%
%\vspace{-7px}

\begin{figure*}
\ifTR{}{\vspace{-10pt}}
\lstinputlisting [language = C, multicols=3] {source_code/scheduling.c}
\ifTR{}{\vspace{-10pt}}
\caption{Pseudocode of selected scheduling primitives.}
\label{fig:exp:sched}
\ifTR{}{\vspace{-5pt}}
\end{figure*}

\noindent{}At this  layer,  scheduling primitives introduce three new events
$c.\yield$, $c.\sleep(i, lk)$
(sleep on queue $i$ while holding the lock $lk$), and $c.\wakeup(i)$ (wakeup the queue $i$).
\ignore{\jieung{In here, we are mentioning about three primitives in here and the next page is mentioning only two primitives for execution switches - right after the next example - I can understand them, but reviewers may confuse about them. - Can we ignore ``wakeup'' in this sentence?}}%
These events record the thread switches,
and the currently-running thread
can be calculated using the replay function $\replay_{\comm{sched}}$.
\ignore{These three events are converted from the events
of multicore toolkit, \ie,
$c.\deq(\comm{pendq}(c))$,
$c.\enq(\comm{slpq}(i))$
and $c.\enq(\comm{pendq}(c))$, respectively.
which indicates the next running thread.
These events encapsulate
the modifications to the 
shared thread control blocks ($\comm{tcbp}$), 
thread queues ($\comm{tdqp}$), 
and the running thread \allid{} ($\comm{tid}$).
Thus, the corresponding abstract states
are hidden at $\Lbthread$ but
can always be reconstructed
by the replay function $\replay_{\comm{sched}}$ given the current log.}
\ignore{The replay function $\replay_{\comm{sched}}$
is defined inductively.
Here we only present the case for the $\sleep$ event:

\ifTR{}{\vspace*{-1.5ex}}
\begin{small}
\begin{mathpar}
\inferrule{
\replay_{\comm{sched}} (l, c) = (\comm{tid}, \comm{tdqp}, \comm{tcbp}) \\
\comm{tdqp} (\comm{rdq}(c)) = r\cons q \\
\comm{tdqp} (\comm{slpq}(i)) = q' \\
\comm{tdqp}'  = \comm{tdqp}
\set{\comm{rdq}(c): q}
\set{\comm{slpq}(i):q'\cons \comm{tid}}\\
\comm{tcbp}'  = \comm{tcbp}
\set{\comm{tid}: \comm{SLEEP}}
\set{r:\comm{RUN}}
}{
\replay_{\comm{sched}} (l \cons c.\sleep (i), c) = (r, \comm{tdqp}', \comm{tcbp}') 
}
\end{mathpar}
\end{small}}%

\ignore{
The abstract states for the shared queues manipulated by yield, sleep, and
wakeup can be reconstructed by the log reply function for the queues applied
on corresponding scheduling events.
}

\ignore{
Since the mapping is one-to-one, the transformation functions for
the log ($f_{l4}$) and environment context ($f_{\oracle4}$)
can be defined trivially.
\david{I'm pretty lost here: is there an explanation anywhere of what 
the parameter $i$ is for the $\sleep$ event? Is it the same as $q$ in
Fig 5? If so, can we make the naming consistent?}
\david{Also, there's a parameter $l$ in Fig 5 that doesn't seem to show up 
in any of these inference rules. I assume this $l$ is completely different
from the logical $l$ used in the rules?}}



\ifTR{}{\vspace{-5pt}}
\subsection{Multithreaded Layer Interface}
\label{sec:multi-threaded-partial}
The CPU-local layer  $\Lbthread[c]$
captures the execution of the whole
thread set of CPU $c$ and does not support thread-local reasoning.
Ideally, we would like to reason about each thread 
separately and later formally compose the proofs together obtaining a global
property for all the threads.
To support this, we need a machine model that gives semantics to
a partially-composed set of threads.

Let $T_c$ denote the whole thread set running over CPU $c$.
From a CPU-local layer  $\PLayer{L}{c}{}$,  we construct a 
 \emph{multithreaded} layer $\TLayer{L}{c}{T_A} := (\PLayer{L}{c}{},
 \Rely^t, \Guard^t)$,
which is 
parameterized over an active thread set $T_A \subseteq T_c$.
The rely condition $\Rely^t$ defines a set of acceptable thread contexts
$\oracle^t$ and the guarantee condition $\Guard^t$ specifies the events generated by active threads. Since our machine model does not allow
preemption, $\oracle^t$ will only be queried during the execution of scheduling primitives, which have two kinds
of behaviors  depending on whether the \emph{target
thread} is active or not.
%\vspace{-5pt}
\[
\includegraphics[width=.50\textwidth]{figs/thread2}
\]%
%\vspace{-12px}

\noindent{}Consider an execution like the one above, with active thread set
$T_A = \{0,1\}$. Whenever an execution switches (by $\yield$ or $\sleep$) 
to a thread outside of $T_A$ (i.e., the yellow $\yield$ above),
it takes environmental steps (i.e., inside the red box), repeatedly appending the 
events returned by the environment context $\oracle$ and the thread
context $\oracle^t$ to the log until a $\yield$
event indicates that the control has switched back to an active thread.
Whenever an execution directly switches to an active thread (i.e., the blue $\yield$ above), 
it will  perform the context switch without invoking $\oracle/\oracle^t$, whose behavior  is identical to the one of $\Lbthread[c]$.

\ignore{
\ronghui{The following paragraph is obsolete}
[[[The machine state of $\TAsm$ is defined as $s:=(\tid, f_\regs, m, f_a, l)$.
Here, $\tid$ is the current thread ID, and
$f_\regs$ and $f_a$ are partial functions
from IDs of threads in $A$ to a register set $\regs$
and a per-thread abstract state $a$, respectively (\ie,
we divide the register set and the abstract state
into separate ones for each thread).
The relation between abstract states
can be easily defined, but the relation of the register set between $L_5$ and $L_6$ is non-trivial.
For the currently-running thread, its local register set ($s_6.f_\regs(s_6.\tid)$) is equal
to the register set of $L_5$ ($s_5.\regs$).
For other threads of $L_6$, the local register set is equal
to the corresponding saved kernel context of $s_5$ for the registers
$[\comm{ra}, \comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]$.
In other words, the register set is decomposed in a way such that the $\regs$ of the
currently-running thread is equal to the machine's register set, while the $\regs$ of
other threads is equal to the corresponding kernel context saved by context switch.
\ignore{
However, $m$ is using
the CompCert memory model \cite{leroy08},
which is not compositional \ronghui{reference here?Tahina?}.
We introduce a new \emph{algebraic memory
model} to make the CompCert memory model
modular (\cf Sec.~\ref{sec:comp} for more details).
Two machines are composable only if they have the same
logical log $l$.
}
For instructions and primitives that do not change the current thread
ID, their semantics are lifted from $L_5$ to $L_6$
by instead operating on the state $(f_\regs(\tid), m, f_a(\tid), l)$.
The scheduling functions (which may change the thread ID to a thread outside of $A$)
are part of the definition of $\TAsm$
and may trigger environment steps by generating events
that will switch control to a thread outside of $A$.
For instance,
the $\yield$ operation of $\TAsm$
will generate a $c.\yield$ event that will switch control to the next thread
and set the machine's $\tid$ accordingly.
If the new thread is in $A$,
then the $\TAsm$ machine will simply keep running local steps of the new thread.
If the new thread is outside of $A$,
then the machine will take environment steps
until the log indicates that control switches back to  $A$.]]]}


\para{Composing Multithreaded Layers.}
Multithreaded layer interfaces with disjoint active thread sets
can also be composed in parallel (using an extended \textsc{Pcomp} rule) if the guarantee condition implies
the rely condition for every thread.
\ignore{
If the active thread sets $T_{A1}$ and $T_{A2}$ of two $\TAsm$ machines
are disjoint, they can be composed together 
to form a machine with the union active thread set $T_{A1} \bigcup T_{A2}$.}%
In this way, the active thread set is the union of the composed ones,
and some environmental steps of one active thread set
are ``replaced by'' the local steps of the other.
For example, if we compose $T_A$ in the above example
with thread 2, the previously yellow $\yield$ of thread 0 will switch to
an active thread instead. 
%\vspace{-5pt}
\[
\includegraphics[width=.50\textwidth]{figs/thread3}
\]%
In this example, the event list $l_1$ generated by $\oracle$ and $\oracle^t$
has been divided into two parts: $l_{1a}\cons c.\yield$ generated by thread 2,
and $l_{1b}$ containing the events generated by threads 
outside \{0,1,2\}.
\ignore{
\david{I don't understand this sentence. If the environment
switched back to thread 2 before switching to thread 1, then
wouldn't there be more than two parts that form $l_2$?}
\newman{In the second figure above, we should use $\oracle'$ instead of $\oracle$
and indicate that $\oracle$ in the first figure is $\oracle'$ plus thread 2. Implicit description
above with ``smaller'' is confusing. }
\jieung{Agree with Newman's suggestion, if we fix the figure by indicating $oracle$ and $oracle'$ (with the explanation that $oracle$ is for thread 0 and 2), we may be able to avoid confusing sentence, and confusing word - ``smaller''}
}

Thanks to this parallel composition rule, given each multithreaded layer $\TLayer{L}{c}{t}$
with a single active thread $t\in T_c$,
we can repeatedly compose them together
and build a layer for the entire thread set on $c$.


\para{Multithreaded Linking.}
When the whole $T_c$  is active,
 all scheduling primitives fall into the second case and never switch to inactive threads. In this case, its scheduling behaviors are equal to the ones of $\Lbthread$. By introducing a multithreaded layer $\Lhthread$
 that contains all the primitives of $\Lhthread$, we can prove
the following theorem:
\begin{theorem}[Multithreaded Linking]
\label{thread_composition}
\begin{small}
$\ltyp{\PLayer{\Lbthread}{c}{\oracle}}{\id}{\varnothing}{\TLayer{\Lhthread}{c}{T_c}}
$%
\end{small}
\end{theorem}%
\noindent This theorem guarantees that,
once the multithreaded machine $\TAsm$ captures
the whole thread set,
 the properties of  threads running on top
can be propagated down to the layer with concrete
scheduling implementations.

\ignore{
\begin{lemma}
{\small
$
\PLayer{\TAsm}{T_c}{\oracle}
\Refrel
\bigJoin_{t \in T_c}
\PLayer{\TAsm}{t}{\oracle}
$}
\label{thread_compose}
\end{lemma}}
\ignore{
\noindent\david{1.) Why is there an ``id'' subscript here for the refinement operator?
2.) What happened to the $\oracle$ parameter of $\TAsm$?}

\newman{Is that linking operator defined explicitly before?}}

%-----------------------------
\ignore{
For scheduling functions (which may change the thread ID to one for a thread not in $A$),
it may trigger the environment step by querying the environment
context $\oracle_T$, which capture the behavior of \emph{inactive} threads (\ie, the threads in $T_c - A$).

We define the specifications of scheduling functions using $\oracle_T$.
For example, the $\yield$ primitive at $L_6$ is specified as:

\vspace*{-1.5ex}
\begin{small}
\begin{mathpar}
\inferrule{
l_0 = l \cons \oracle (l) \\
(l', \tid') =  \comm{yield\_back}
(A, \oracle_T, l_0 \cons c.\yield) \\
f_\regs' = f_\regs\set{\tid': 
\text{undefined value except for }[\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]}
}{
 A, \oracle_T, \oracle, c\vdash_{\comm{Asm}} \sstep{\spec_{\yield'}}{[]}{\tid,f_\regs, m, f_a, l}{\tid', f_\regs',  m, f_a, l'}
}
\end{mathpar}
\end{small}%

Note that $\yield$ changes the $\tid$ to $\tid'$.
After that, the execution operates on
the state $f_\regs'(\tid')$ and $f_a(\tid')$.
When $\tid'$ is different from $\tid$, this simulates the behavior of context switch
within the active thread set $A$. 
If the active thread set $A$ is $T_c$, we have:
}

\ignore{
If the active thread set $A$ is the whole
thread set $T_c$, we have:

Then, we can prove the simulation
relation:
\begin{lemma}[$\LAsm$ refines $\TAsm$]
\label{thread_composition}
{\small
\[
\forall \oracle,~
\LAsm(L_5(c,\oracle)) 
\refines
\TAsm(L_6(c,\oracle, T_c, \any))
\]}%
\end{lemma}



\begin{lemma}[CPU-local machine refines multi-thread machine]
\vspace{-5px}
{\small
\[
\forall M~ \oracle, 
\sem{ }{M}L_5(c,\oracle) \Refrel_{R_5} 
\sem{}{M}L_6(T_c, \oracle)
\]}
\label{whole_thread_composition}
\end{lemma}}

\ignore{
\textbf{Proof sketch}:
For whole thread set,
the environment context only carries
the information of the context CPUs.
Thus, $\oracle_{T_c} = \oracle$.
For the simulation relation $R_5(s_5,s_6)$, 
the only non-trivial part is the relation
for register set function $s_6.\regs p$.
For the current thread,
$s_6.\regs p(s_6.\tid)= s_5.\regs$.
For other threads,
the register set of $s_6$ is equal
to the kernel context of $s_5$
for the register list
$[\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}]$.
\ignore{
which is defined as:
1) ;
and 2)$\forall i \neq s_6.\tid,
s_6.\regs p(i)= s_5.a.\comm{kctxt}$.}
}
%-----------------------------

\subsection{Thread-Local Layer Interface} \label{subsec:phthreadlayer}
As a special case, when a multithreaded layer $\TLayer{L}{c}{t}$ 
only has  a single active thread $t\in T_c$,
 the $\yield$ and $\sleep$ primitives  always
switch to an inactive thread and then repeatedly query $\oracle$ and $\oracle^t$
until yielding back to $t$.
%\vspace{-5pt}
\[
\includegraphics[width=.50\textwidth]{figs/thread4}
\]%
%\vspace{-17px}
At this special layer, the scheduling primitives
always  end up switching back to the same thread.
These ``self-switches'' do not actually modify the kernel context 
($\comm{ra},
\comm{ebp}, \comm{ebx}, \comm{esi}, \comm{edi}, \comm{esp}$), 
such that the C calling convention is satisfied.
Thus, the scheduling primitives at this layer
are defined in the C-style as shown below:
\begin{small}
%\vspace{-5px}
\begin{mathpar}
\inferrule{
l_0 = \Query{\oracle}{l}{c}\\
l' =  \comm{yield\_back}
(\tid, \oracle^t, \oracle, l_0 \cons c.\yield)
}{
 \tid, \oracle^t, c, \oracle \vdash \sstep{\spec_{\yield}}{[]}{\regs, m, a, l}{\regs, m, a, l'}
}
\end{mathpar}
%\vspace{-12px}
\end{small}%
Here, the auxiliary function $\comm{yield\_back}$ 
specifies the behavior of repeatedly querying 
$\oracle$ and $\oracle'$ until the control flow yields back to the thread of interest $\tid$.
It appends all the events triggered by the inactive threads and inactive CPUs to the log.
Since our specifications are termination-sensitive,
we prove that $\comm{yield\_back}$ actually terminates,
by showing that the software scheduler is \emph{fair} and every running
thread gives up the CPU within a finite number of steps.
We call $\TLayer{L}{c}{t}$ a ``thread-local'' layer 
because the scheduling primitives
do not switch control to other threads;
they effectively act as a ``no-op'',
except that the shared log gets updated.
On top of this thread-local layer,
we can verify the thread execution
locally by building higher-level thread local layer interfaces.
% ------------------------------------------------------------------
% Let's not talk about linking partial machines here
\ignore{
By Lemma~\ref{lemma:mono},
per-thread layers can be composed using
``$\Join$'', and propagated down to the $L_6$ machine.
Once the layers are composed for the full thread set $T_c$, we can apply Lemma~\ref{thread_composition}
to transfer the guarantee down to $L_5$
and link with per-CPU layers.
}
% ------------------------------------------------------------------
\ifTR{}{\vspace{-5pt}}
\subsection{Queuing Lock}\label{subsec:qlockimplementation}

% \begin{wrapfigure}{r}{0.49\textwidth}
\begin{figure}[t]
\ifTR{}{\vspace{-20pt}}
\lstinputlisting [language = C, multicols=2] {source_code/queue_lock.c}
\ifTR{}{\vspace{-17pt}}
\caption{Pseudocode of queuing lock.}
\label{fig:exp:qlock}
\ifTR{}{\vspace{-8pt}}
%\end{wrapfigure}%
\end{figure}
Based upon the thread-local layer,
we can build additional synchronization toolkits, such as
a queuing lock (\cf Fig.~\ref{fig:exp:qlock}).
With  queuing locks, waiting threads are put to sleep to avoid busy spinning.
Reasoning about this locking algorithm is particularly
challenging since its C implementation utilizes both
spinlocks and low-level scheduler primitives (\ie, $\sleep$ and $\wakeup$).
We have to 
introduce multiple layer interfaces on top of $\TLayer{\Lhthread}{c}{t}$ to verify this queuing lock implementation.

Similar to the spinlock,
the verification of a queuing lock consists of
two parts: mutual exclusion and starvation freedom.
The lock implementation is mutually exclusive
because the busy value of the lock (\texttt{ql\_busy})
is always equal to the lock holder's thread \allid{}.
This busy value is set either
by the lock requester when the lock is free (line 6 of 
Fig.~\ref{fig:exp:qlock})
or by the previous lock holder when releasing the lock
(line 12).
With the atomic interface of the spinlock, the starvation-freedom proof of queuing lock
is mainly about the termination of the sleep primitive call
(line 4). By showing that all the lock holders
will eventually release the lock,
we prove that all the sleeping threads will be 
added to the pending queue or ready queue within a finite number
of steps. Thus, the sleep primitive call will return
thanks to the \emph{fair} software scheduler.
Note that all these properties proved at the C level can be propagated down to the assembly level using the thread-safe CompCertX.

% ------------------------------------------------------------------
\ignore{
\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/high_example.c}
\ifTR{}{\vspace{-5pt}}
\caption{Pseudocode of a simple producer-consumer example.}
\label{fig:exp:high}
\vspace{-10pt}
\end{figure}

\subsection{End-to-end example}\label{subsec:end-to-end-example}
In this section, we show how to reason about a simple
producer-consumer example (\cf Fig.~\ref{fig:exp:high}) based on the toolkit  we have built.
Suppose thread $t_0$ on CPU~0 is the producer,
which simulates the resource generation by increasing a counter by 2
(line 7 in Fig.~\ref{fig:exp:high}).
This shared counter is synchronized using a queuing lock.
Besides, there are two consumers, threads $t_1$ and $t_2$ on CPU~1,
which wait until the resource is available (lines 13-17),
and then consume it by decreasing the counter.

Although this example looks pretty simple,
reasoning about the behavior of the whole system
and propagating the guarantee down to concrete code
running on multicore hardware are extremely challenging.
This verification task involves too many abstraction levels,
such as the multicore concurrency, multithread concurrency,
and the queuing lock synchronization implemented with
spinlocks.

Thanks to our toolkit, we can not only reason about this example,
but also do it in a thread-local way.
On the thread-local layer, we show that the producer 
code meets the specification that generates a single
event ``$t_0.(ct +2)$'', which is converted
from 
two queuing lock events on CPU~0 ``$0.\acq\_\comm{q}(lct, ct)$'' and ``$0.\rel\_\comm{q}(lct, ct+2)$'' at the lower layer, where $ct$ stands for
the counter value.
\ignore{ \begin{small}
\begin{mathpar}
\inferrule{
l' = \Query{\oracle}{l}{c} \cons \tid.(ct+2)
}{
 \tid, c, \oracle \vdash   \sstep{\spec_{\texttt{producer}}}{[]}{\regs, m, a, l}{\regs, m, a, l'}
}
\end{mathpar}
\end{small}}%
\ignore{\jieung{maybe, $lct\text{+2}$ should be changed as $ct\text{+2}$ in the rule}}%
Since $t_0$ is the only thread  running on CPU~0,
the behavior of this processor is defined as the set of
logs that the only event  from CPU~0  is ``$t_0.(ct\text{+2})$''.
\david{I don't follow this paragraph at all. Is it necessary?}

The specification of the consumer keeps 
querying $\oracle$ until the counter is positive
\ignore{(denoted below as the function $\texttt{wait\_pos}$)}
and generates an
event $``\tid.(ct-1)''$.
This event is converted from
a sequence of events representing both the waiting part
($1.\acq\_\comm{q}(lct, ct)$, $\yield$, and $1.\rel\_\comm{q}(lct, ct)$),
as well as the events for resource consumption:
$1.\acq\_\comm{q}(lct, ct)$ and $1.\rel\_\comm{q}(lct, ct - 1)$.
\ignore{ \begin{small}
\begin{mathpar}
\inferrule{
l_0 = \texttt{wait\_pos}(l, \oracle) \\
l' = l_0 \cons \tid.(ct -1)
}{
 \tid,  c, \oracle \vdash   \sstep{\spec_{\texttt{consumer}}}{[]}{\regs, m, a, l}{\regs,  m, a, l'}
}
\end{mathpar}
\end{small}}%
Here we pose an invariant that the context CPUs will produce enough resources
to guarantee the termination of consumer functions.
Thus, after composing threads $t_1$ and $t_2$,
the behavior of CPU~1 contains all the logs
that there are only two events ``$t_1.(ct -1)$''
and ``$t_2.(ct -1)$'' from CPU~1 and the counter is always non-negative.
Therefore, after composing two CPU together,
the behavior of the whole machine
consists of two logs: ``$t_0.(ct +2)\cons t_1.(ct -1) \cons t_2.(ct -1)$'' and ``$t_0.(ct +2)\cons t_2.(ct -1) \cons t_1.(ct-1)$''.
\david{Again, I'm really not following this notion of defining a behavior
with a set of logs. Was this described earlier in the paper?}

We could easily derive properties of the whole
machine over this log set behavior,
such as the counter is always non-negative, the counter is 0 in the end, etc.
These properties can be propagated down to the
execution of assembly code on x86 multicore machine 
by the toolkit layers we have built so far.
This log set is also converted from a set of logs consisting
of lowest-level events  (i.e., $\incticket$,
$\getnow$, $\incnow$, $\holdlock$, $\push$, $\pull$),
which would be extremely difficult to reason about directly.}

\ignore{
\paragraph{Queuing Lock} is an algorithm whereby waiting threads are put to sleep,
so that busy waiting is avoided.
Verification of this complex locking algorithm is particularly
challenging since its C implementation utilizes both
spinlocks and low level scheduler primitives ($\sleep$ and $\wakeup$).
In our framework, a queuing lock implementation is verified
by building multiple layer interfaces on top of $L_7$.
}

\ignore{
We first introduce two atomic operations (using $\comm{CAS}$)
that set and clear the busy bit of the lock.
Then we prove that the implementations
of the lock and unlock operations
satisfy their atomic specifications
(which generate single $\comm{acq\_q}$ and $\comm{rel\_q}$ events, respectively).
This is achieved by proving a simulation between layer interfaces together
with starvation-freedom, using a strategy similar to verification of
the ticket lock implementation.
}

\ignore{

 is a general
lock algorithm that 
allows sleeping for the lock, instead of busy waiting.
Since queuing lock
relies on the spinlocks and thread scheduling
and is implemented at C-level
(\cf Fig.~\ref{fig:exp:queue_lock}),
it is hard to reason about
and has never been verified in previous works.

In our framework, queuing lock can be easily verified
based on $L_7$.
We first introduce two atomic operations
to set the busy bit if the lock is free (\ie,
$\comm{CAS\_qlock}$, implemented
using $\comm{CAS}$),
and to clear the busy bit (\ie, $\comm{clear\_qlock}$).
Then, we prove the implementation 
of lock and unlock operations
satisfy the atomic specifications
(\ie, $\comm{acq\_q}$ and $\comm{rel\_q}$ events)
by showing the simulation relation
and the starvation-freedom.
The verification details are similar to the ticket lock.

\begin{figure}
\lstinputlisting [language = C, multicols=2] {source_code/queue_lock.c}
\ifTR{}{\vspace{-5pt}}
\caption{Pseudocode of queuing lock implementation.}
\label{fig:exp:queue_lock}
\ifTR{}{\vspace{-10pt}}
\end{figure}

}
