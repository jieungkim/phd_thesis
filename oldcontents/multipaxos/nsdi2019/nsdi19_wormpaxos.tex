\section{\sysname{} Applications}
\nsdinew{
%To illustrate how \sysname{} supports application designs we present WormPaxos, WormLog and WormTX. 
%To illustrate what \sysname{} can provide for designing applications we present WormPaxos, WormLog, and WormTX.
To illustrate how \sysname{} simplifies applications, we present WormPaxos, WormLog, and WormTX.}



\subsection{WormPaxos over \sysname{}}

\begin{figure}
\centering
\includegraphics[page=2]{pics/pics-small.pdf}
\caption{WormPaxos: servers replicate state by ordering proposals on the WormSpace address space.\label{fig:paxosarch}}
\vspace{-0.1in}
\end{figure}

In principle, implementing Multi-Paxos over \sysname{} is simple: the sequence of commands is stored on the \sysname{} address space. WormPaxos is an implementation of Multi-Paxos over \sysname{}, exposing a conventional state machine replication API to applications. In WormPaxos, servers that wish to replicate state act as \sysname{} clients; we call these \WPservers{}. They can \api{propose} new commands by preparing and writing to the next free address in the \sysname{}; and \api{learn} commands by reading the address space in sequential order. If a proposing client finds that the current tail is at the end of a \WOS{}, it allocates a new one and then writes to the next address.
\worry{what about over-allocation due to concurrent allocation with a sequential alloc interface?}

The chief benefit of this layered design is extreme simplicity; the Multi-Paxos consists of a few hundreds of lines of code, which calls data-centric commands over the \sysname{} address space. This design also enables flexibility along a number of dimensions (Figure~\ref{fig:paxosarch}):\cuttext{ compared to conventional Multi-Paxos designs, matching the performance of monolithic designs while providing new capabilities. Specifically:}


\textbf{Flexible Consensus} \textit{(i.e., how is the \WOR{} implemented?)}: Consensus in WormPaxos is hidden under the WOR abstraction and can be implemented via many different protocols, ranging from variants of Paxos, atomic broadcast protocols such as ZAB, and protocols such as Primary-Backup and Chain Replication. In contrast, existing Multi-Paxos designs weld together the single-decision consensus engine -- typically Paxos -- with the state machine replication machinery responsible for consistency and availability. 
For example, the WormPaxos codebase can run with zero modification over a \WOR{} implementation based on Chain Replication rather than Paxos; in contrast, existing Multi-Paxos implementations require extensive modification to run over a different single-shot consensus protocol.
\thoughts{Can we show performance for the system with two different protocols: chain replication versus paxos.}

\textbf{Flexible Leadership} \textit{(i.e., who calls \api{\prepare{}}?)}: Sticky leadership -- i.e., retaining a single leader across multiple commands -- is a key performance imperative for Multi-Paxos implementations, since it A) allows commands to be decided within a single round-trip rather than two in the absence of failures, and B) eliminates contention between leaders. In many Multi-Paxos implementations, leadership strategy is baked into the system design; for example, Raft~\cite{raft} is explicitly designed to support sticky leadership as a first-class consideration. In WormPaxos, a \WPserver{} becomes a sticky leader simply by using a batch \api{\prepare{}} on a \WOS{}; accordingly, leadership strategies such as sticky leader, rotating leader, etc. can be implemented simply as policies on who should call the batch \api{\prepare{}} and when. Further, the leader's identity can be stored within the metadata for each segment, obviating the need for \sysname{} to know about the notion of a leader or the leadership strategies involved.
\thoughts{We can show the performance of different leadership strategies and compare with existing versions.}

\textbf{Flexible Durability} \textit{(i.e., when is \api{trim} called?)}: By varying when it calls \api{trim}, WormPaxos can employ different strategies for durability. For instance, a \WPserver{} can \api{trim} a prefix of the \sysname{} as soon as a certain number of \WPservers{} have seen it, or some \WPserver{} has stored a snapshot in an external data store; this information can be piggybacked on new commands appended to the address space. In contrast, existing Multi-Paxos designs are tied to a particular strategy for durability (e.g., when all replicas have seen a command~\cite{rvrpaxos}).
\thoughts{This can be qualitative?}
%Alternatively, a \WPserver{} can store a snapshot of its state in some external data store before calling \api{trim}

\textbf{Flexible Consistency} \textit{(i.e., what addresses do we \api{write} and \api{read}?)}: WormPaxos derives consistency properties such as linearizability, sequential consistency, or eventual consistency via strategies for writing/reading to the address space. The state at each \WPserver{} reflects some subset of updates in the \sysname{}. For linearizable writes and reads, each command has to locate a slot after any completed writes in the address space, but before any empty slots that could be filled by later commands. For a weaker guarantee such as sequential consistency, \WPservers{} can allocate separate segments and write to them in parallel. Similarly, causal consistency can be obtained by ensuring that new writes from a \WPserver{} go to a later address than any it has already seen. For these weaker consistency guarantees, the random write / random read nature of the \sysname{} API allows us to parallelize proposing in a way that we could not do over a conventional SMR (sequential write / sequential read) or shared log (sequential write / random read) interface. 
\thoughts{Sequential consistency?}

%; each \WPserver{} can simply ensure that it does not play back commands beyond the \WOS{} it is currently writing to
%On a read, a \WPserver{} locates the next unallocated segment via \api{check} calls, and then plays the commands in the address space until that point, either waiting for each address to be written or filling it with a junk value. 

%A conventional sticky leader design -- where all \WPservers{} send writes to a single leader, which allocates \WOSes{}, issues batch captures, and performs the writes -- suffices. 


\cuttext{For linearizability, the total order of writes imposed by the address space has to reflect real-time order (i.e., if an update $U_1$ completed before another update $U_2$ started, then the position of $U_1$ has to be earlier in the address space than $U_2$.).} 

\cuttext{Note that the sticky leader is allowed to issue multiple outstanding writes to consecutive addresses in the \WOS{}; this can result in concurrent commands being ordered arbitrarily, which does not violate linearizability.}

%To support linearizable reads, a \WPserver{} has to place a marker in the address space and synchronize up to that point. All completed writes must exist before this marker in the address space. To obtain such a marker, the \WPserver{} can contact the sticky leader. Alternatively, it can simply locate the next free \WOS{} and wait until the preceding \WOS{} is filled. In practice, we use a trick employed by Percolator~\cite{percolator} and Tango~\cite{tango} where we queue reads on the replicated data structure behind a single outstanding `sync' operation that places a marker in the address space and synchronizes. As long as each read waits for the completion of a sync that started after it, we obtain linearizability for reads.


