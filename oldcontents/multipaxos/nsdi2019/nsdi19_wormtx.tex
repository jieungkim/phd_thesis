\subsection{WormTX over \sysname{}}

Two-Phase Commit (2PC)~\cite{distsys} solves the transaction commit problem via a transaction manager (TM). Any participant (RMs, or resource managers) that wishes to initiate a commit contacts the TM (message delay \#1). The TM contacts all participants to elicit a yes/no vote (\#2). Each RM votes, records its vote in local stable storage and responds to the TM (\#3). The TM makes a decision based on the votes it receives, and sends back a commit or abort command to the RMs (\#4). The TM's decision can be a deterministic function of the RM votes -- i.e., the decision is yes if all the votes are yes. Alternatively, the TM can decide no even if all the votes are yes, in which case it stores its decision in stable storage before sending the decision.

The failure model for 2PC is that nodes -- TMs or RMs -- can crash, but will subsequently come back online. 2PC is known to be a blocking protocol in the presence of such failures. In the case where the decision is deterministic, if a single RM fails -- after it has locally stored its vote in stable storage, but before it has responded to the TM -- then the protocol has to block until the RM comes back online. In the case where the TM fails -- after storing its final decision in stable storage but before sending commit messages -- the protocol has to block until the TM comes back online. In both cases, the remaining RMs cannot determine the decision. 

%If a RM fails in 2PC, the TM can simply time-out and abort the transaction. However, if the TM fails after collecting all the votes but before sending out its decision, the protocol blocks; RMs have no way to determine what the decision was.

\label{sec:wormtx}

\begin{figure}
\centering
\includegraphics[page=5]{pics/pics-longer.pdf}
\caption{WormTX: \WOR{}-based non-blocking atomic commit protocols. Dashed arrows are notifications. \label{fig:commit}}
\vspace{-0.1in}
\end{figure}


We consider making the deterministic (i.e., the TM does not have a separate vote) version of 2PC non-blocking. We come up with a number of variants that use \WORs{}. We describe them below and in Figure \ref{fig:commit}.

\textbf{[Variant A8: 8 message delays]} An obvious solution is to simply store the votes in a set of per-RM \WORs{}. If the TM decision is non-deterministic, a \WOR{} is used to store the decision as well. In the \WOR{}-based 2PC protocol, an RM initiates the protocol by contacting the TM (message delay \#1); the TM contacts the RMs (\#2); they \prepare{} the \WOR{} (\#3 and \#4), and then write to it (\#5 and \#6); send back their decision to the TM (\#7), which sends back a commit message to all the RMs (\#8). This corresponds exactly to using Paxos as a black-box.% fashion.% to durably store the votes.

%The number of messages is 1 + N + 4(F + 1) + N + N = 3N + 1 + 4(F+1)

\textbf{[Variant B6: 6 message delays]} A simple optimization involves eliminating the \prepare{} messages from the critical path. Each RM can allocate a dedicated \WOS{} for its decisions and batch \prepare{} the \WOS{} in advance. This eliminates delays \#3 and \#4 from variant A8.%, bringing us down to 6 message delays.

%The number of messages is 3N + 1 + 2(F+1) (+ delta for batched \prepare{})

\textbf{[Variant C5: 5 message delays]} Further, rather than have the RM wait for an ACK on the write (message delay \#6 in variant A8) and relay it to the TM (\#7 in A8), the TM can directly observe the decision by listening for write notifications on the \WOS{}. This compresses \#6 and \#7 of variant A8 into a single step.%, bringing us down to 5 message delays. 

%The number of messages is 3N + 1 + 2(F+1) + (F+1)N. If we have batched notifications, the final term is just F+1.

\textbf{[Variant D4: 4 message delays]} Finally, rather than have the TM wait to be notified of all the \WOR{} writes and then send out a commit message to all the participants (\#8 of variant A8), individual RMs can directly listen to each other's \WOSes{}; this brings us down to 4 message delays.

%The number of messages is 2N + 1 + 2(F+1) + (F+1)*N*N. If we have batched notifications, the final term is just (F+1)N.

%2N + 1 + 2F + 2 + (F+1)N
%= 2N + 3 + 2F + NF + N
%= 3N + 3 + 2F + NF
%= 3(N+1) + F(N+1) + F

This progression of increasingly fast protocols exactly matches the description by Gray and Lamport~\cite{gray:2006}; they too proceed from an unoptimized 8-step protocol to an optimized 4-step one in identical fashion, via 6-step and 5-step protocols. In their case, this is achieved by opening up the Paxos protocol and rewiring the flow of requests and ACKs between the various Paxos roles of acceptors, leaders, proposers, and learners. In our case, the optimizations are achieved via the \sysname{} API, without requiring any knowledge of the Paxos protocol.

\textbf{[Variant E3: 3 message delays]} We now observe that we do not need a TM, since the final decision is a deterministic function of the \WORs{}, and any RM can time-out on the commit protocol and write a no vote to a blocking RM's \WOR{} to abort the transaction. The initiating RM can simply contact the other RMs on its own to start the protocol (combining \#1 and \#2 of variant A8), bringing down the number of delays to 3. Interestingly, this variant is not described by Gray and Lamport.

%The number of messages is 3N + 2(F+1) + (F+1)*N*N

\textbf{[Variant F2: 2 message delays]} Finally, if RMs can `spontaneously' start the protocol and vote, we eliminate delays \#1 and \#2 of variant A8, bringing the protocol down to two delays, the theoretical minimum for atomic commit. Since this is not a realistic assumption for many systems, we choose variant E3 as our final solution.

\nsdinew{
Our protocol is in contrast to other non-blocking commit protocols, which require complex message passing logic~\cite{distsys}. Instead, we assemble a non-blocking protocol via simple, intuitive, and data-centric commands on \WORs{}.
}

%Our protocol is in contrast to other non-blocking commit protocols such as 3-phase commit~\cite{distsys}, which require complex message passing logic. Instead, we assemble a non-blocking protocol via simple, intuitive, and data-centric commands on \WORs{}.

%\subsubsection{Concurrency control}
%\label{ssec:wormtx-cc}
%WormTX allows for pluggable concurrency control by allowing the application to determine the steps it takes within the voting / commit steps of two phase commit. 
\nsdinew{
\textbf{Concurrency Control:} The proposed atomic commit schemes can be integrated with concurrency control schemes based on timestamps, deadlock detection, etc. We implemented a simple concurrency control protocol based on locking that uses Immediate-Restart~\cite{agrawal1987} for deadlock prevention.}

\nsdinew{
Consider variant E3. The server that performs a transaction notifies all servers involved. Each server tries to acquire a lock on its local data for the transaction. If it succeeds, the server writes a write-ahead log and then a yes vote to its \WOR{}. Upon failure to lock, the server immediately aborts the transaction by writing a no vote to its \WOR{}.}

\nsdinew{
If each server receives yes ACKs for its own yes write from all servers involved, it updates the data and releases the lock. Otherwise, it releases the lock without the update. This protocol provides strict serializability and failure atomicity.}


%\nsdicmt{removed details for concurrency control.}

%So far we have described a non-blocking atomic commit protocol built using \WORs{}. To implement distributed transactions with transactional isolation and atomicity, we need to integrate this protocol with some form of concurrency control. 
% (for example, to add an edge between two nodes of a graph that is stored in the key-value store in adjacency list format)
\cuttext{Consider performing a transactional update on a partitioned key-value store. Using variant E3, a client or one of the servers that wants to perform this transaction sends a message to the two servers involved. Each server attempts to acquire a local lock on the key being modified. If it succeeds, it writes a write-ahead log entry to local storage (in case it crashes and reboots). It then writes a yes vote to its \WOR{}. If the lock is currently held, the server follows the Immediate-Restart policy and aborts the new transaction by writing a no vote to its \WOR{}.
}

\cuttext{
If each server receives an ACK for its own yes write, and a notification that yes has been written to the other \WOR{}, it proceeds with the update and then releases the lock. If a server votes yes but is notified that the other server voted no, then it releases its local lock without performing the update. This simple protocol provides strict serializability as well as failure atomicity.
}
%This simple protocol provides strictly serializable transactional isolation as well as failure atomicity for distributed transactions. A variant of this protocol involves multi-version concurrency control, where a client first executes a transaction by reading versioned state speculatively from the two servers and buffering writes; in the commit protocol, each server votes `yes' if the locks can be acquired and the data read by the transaction is still valid.



%Each RM can then independently determine whether the transaction has committed by checking the set of decision \WORs{}.

%Gray/Lamport
%3.2 talks about 4 delays: A: RM1 to TM; B: TM to RMs; C: RMs to TM; D: TM to RMs.
%4.2 Mohan/Strong/Finkelstein use a consensus algorithm to decide -- how?
%4.2 Paxos Commit uses one Paxos instance for each RM's decision; same acceptors and leaders; 
%4.2 "In ordinary Paxos, a ballot 0 phase 2a message can have any value v. While the leader usually sends such a message, the Paxos algorithm works just as well if, instead of the leader, some other single process sends that message. In Paxos Commit, each RM announces its prepare/abort decision by sending, in its instance of Paxos, a ballot 0 phase 2a message with the value Prepared or Aborted."
%4.2 Paxos Commit protocol: Step A is same (RM1 to TM); step B is same (TM to RMs); new step C: each RM runs phase 2A on acceptors with its decision; new step D: each acceptor sends its phase 2b to TM; Step E: TM sends decision to RMs. 
%Step E can be avoided if acceptor directly sends phase 2b to RMs as well.

%Now if we just layer 2PC on WORs, we get: RM1 to TM; TM to RMs; RMs to acceptors phase 1+2; RMs to TM; TM to RMs commit. 8 message delays. If we use sticky leader, then we get 6 message delays. TM can directly query WORs to get 5 message delays (but how --- we do polling, not notification?). RMs can directly query / be notified by WORs to get 4 message delays. If RMs somehow spontaneously prepare (how?), then it's 2 message delays and optimal.

%point to note: they use the term leader above instead of TM. 

%5 compares paxos commit and 2PC; it points out that 2PC allows TM to abort unilaterally, whereas paxos commit does not. 
