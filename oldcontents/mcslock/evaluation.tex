
\section{Evaluation}
\label{sec:evaluation}

\begin{figure}
\begin{minipage}{\linewidth}
\noindent
\begin{multicols}{2}
\lstinputlisting[numbers = left, language = C]{source_code/mcslock/palloc_example.c}
\lstinputlisting[numbers = left]{source_code/mcslock/sharedeventtype.v}
\end{multicols}
\end{minipage}
\caption{\lstinline$palloc$ Example}
\label{fig:palloc-example}
\end{figure}

\paragraph{Clients}
The verified MCS lock code is used by multiple clients in the CertiKOS
system. To be practical the design should require as little extra work
as possible compared to verifying non-concurrent programs, both to
better match the programmer's mental model, and to allow code-reuse
from the earlier, single-processor version of CertiKOS.

For this reason, we don't want our machine model to generate an event
for every single memory access to shared memory. Instead we use what
we call a \emph{push/pull memory model}~\cite{certikos16,ccal16}. A
CPU that wants to access shared memory first generates a ``pull''
event, which declares that that CPU now owns a particular block of
memory. After it is done it generates a ``push'' event, which
publishes the CPU's local view of memory to the rest of the system. In
this way, individual memory reads and writes are treated by the same
standard operational semantics as in sequential programs, but the
state of the shared memory can still be replayed from the log.  The
push/pull operations are logical (generate no machine code) but
because the replay function is undefined if two different CPUs try to
pull at the same time, they force the programmer to prove that
programs are well-synchronized and race-free. Like we did for atomic
memory operations, we extend the machine model at the lowest layer by
adding logical primitives, e.g. \lstinline$release_shared$ which takes a
memory block identifier as an argument and adds a
\lstinline$OMEME (l:list Integers.Byte.int)$ event to the log, where the byte list is a
copy of the contents of the shared memory block when the primitive was
called.

When we use \lstinline$acquire$/\lstinline$release_shared$ we
need a lock to make sure that only one CPU pulls, so we begin
by defining combined functions \lstinline$acquire_lock$ which
takes the lock (with a bound of 10) and then pulls, and
\lstinline$release_lock$ which pushes and then releases the
lock. The specification is similar to \lstinline$pass_hlock_spec$,
except it appends \emph{two} events.

Similar to Sec.~\ref{sec:liveness-atomicity}, logs for different
layers can use different types of pull/push events.
Fig.~\ref{fig:palloc-example} (right) shows the events for the
\lstinline$palloc$ function (which uses a lock to protect the page
allocation table). The lowest layer in the palloc-verification adds
\lstinline$OMEME$ events, while higher layers instead add
(\lstinline$OATE (a: ATable)$) events, where the relation between logs
uses the same relation as between raw memory and abstract
\lstinline$ATable$ data. Therefore, we write wrapper functions
\lstinline$acquire$/\lstinline$release_lock_AT_spec$, where the
implementation just calls \lstinline$acquire$/\lstinline$release_lock$
with the particular memory block that contains the allocation table,
but the specification adds an \lstinline$OATE$ event.
This refinement step, which changes the log replay function to compute
allocation tables instead of byte lists, is
specific to the \lstinline$palloc$ function.

\begin{figure}

\lstinputlisting{source_code/mcslock/release_lock_AT_spec_short.v}
\lstinputlisting{source_code/mcslock/palloc_spec_short.v}

\caption{Specification for \lstinline$palloc$}
\label{fig:palloc-spec}
\end{figure}

We can then ascribe a low-level functional specification
\lstinline$palloc'_spec$ to the \lstinline$palloc$ function. As shown
in Fig~\ref{fig:palloc-spec}, this is decomposed into three parts, the
acquire/release lock, and the specification for the critical
section. The critical section spec is exactly the same in a sequential
program: it does not modify the log, but instead only affects the 
\lstinline$AT$ field in the abstract data.

Then in a final, pure refinement step, we ascribe a high-level atomic
specification \lstinline$lpalloc_spec$ to the \lstinline$palloc$
function. In this layer we no longer have any lock-related events at
all, a call to \lstinline$palloc$ appends a single
\lstinline$OPALLOCE$ event to the log. This is when we see the
proof obligations related to liveness of the locks.
Specifically, in order to prove the downwards refinement, we need to
show that the call to \lstinline$palloc'_spec$ doesn't return
\lstinline$None$, so we need to show that \lstinline$H_CalLock l'$ is
defined, so in particular the bound counter must not hit zero.
By expanding out the definitions, we see that
\lstinline$palloc'_spec$ takes a log \lstinline$l$ to
\lstinline$REL_LOCK :: (OATE (AT adt)) :: (TSHARED OPULL) :: (WAIT_LOCK 10) :: l$.
The initial bound is 10, and there are two shared memory events, so the
count never goes lower than 8. If a function modified more than one
memory block there would be additional push- and pull-events, which
could be handled by a larger initial bound.

Like all kernel-mode primitives in CertiKOS, the \lstinline$palloc$ function is
total: if its preconditions are satisfied it always returns. So
when verifying it, we show that all loops inside the critical section
terminate. Through the machinery of bound numbers, this guarantee is
propagated to the the while-loops inside the lock implementation:
because all functions terminate, they can know that other CPUs will
make progress and add more events to the log, and because of the
bound number, they cannot add push/pull events forever. On the other
hand, the framework completely abstract away how long time (in microseconds) elapses
between any two events in the log.

\paragraph{Code reuse} The same
\lstinline$acquire$/\lstinline$release_lock$ specifications can be
used for all clients of the lock. The only proofs that need to be done
for a given client is the refinement into abstracted primitives like
\lstinline$release_lock_AT_spec$ (easy if we already have a sequential
proof for the critical section), and the refinement proof for the
atomic primitive like \lstinline$lpalloc_spec$ (which is very
short). We never need to duplicate the thousands of lines of proof
related to the lock algorithm itself.

\paragraph{Using more than one lock}
The layers approach is particularly nice when verifying code that uses more than one
lock. To avoid deadlock, all functions must acquire the locks in the
same order, and to prove the correctness the ordering must be
included in the program invariant. We \emph{could} do such a
verification in a single layer, by having a single log with different
events for the two locks, with the replay function being undefined if
the events are out of order. But the layers approach provides a
better way. Once we have ascribed an atomic specification to
\lstinline$palloc$, as above, all higher layers can use it
freely without even knowing that the \lstinline$palloc$ implementation
involves a lock (Note that the lock is not re-exported from the
\lstinline$palloc$ layer, and if it was the proof of the atomic
specification would not go through.)  For example, some function in a
higher layer could acquire a lock, allocate a page, and release the
lock; in such an example the the order of the layers provides an order
on the locks implicitly.




\ignore{

Our verified MCS Lock is actually used by multiple clients in CertiKOS system. Fig.~\ref{fig:palloc-example} (left) is one example of client code. Each client code that uses the lock defines its own lock wrappers. In the example, \lstinline$acquire_lock_AT$ and \lstinline$release_lock_AT$ are the lock primitives that are defined using the atomic lock specifications (\lstinline$wait_hlock_spec$ and \lstinline$pass_hlock_spec$, respectively), and they are only designed to the page allocation table protection. 

Using the lock with client codes also requires a way to add shared memory events into the log. They are essential in our system because replaying the log should reflect the status of global shared memory as well as the lock status. 
Those events may be different in the different layers because of the similar reason in Sec.~\ref{sec:liveness-atomicity}. Fig.~\ref{fig:palloc-example} (right) shows the events that are for \lstinline$palloc$, and we can refine logs that are using different types of shared memory events using the approach that we mention in Sec.~\ref{sec:verification}.

\ignore{Our MCS Lock verification is actually used with several client codes in CeritKOS system, and we briefly discuss how our MCS Lock can be associated with them by using the simplest example, \lstinline$palloc$ in Fig.~\ref{fig:palloc-example}.}

First step is introducing the \textit{logical} machine-level primitive 
that can memorize the actual memory values associated with shared memory in the log. 
In this sense, we have introduced \lstinline$release_shared$, 
a logical primitive in the lowest layer in our system.
The primitive will add \lstinline$OMEME (l: list Integers.Byte.int)$ event to the log, 
which mirrors the current local copy of the shared memory. 
However, this does not mean that the operation on shared memories generate events at all, 
and memorizing the current shared memory status in the log when the system release the lock.
Thus, mapping this \lstinline$release_shared$ primitive with 
our MCS Lock verification is the second step to use the verified lock code with client codes.
After introducing the atomic lock specification (\lstinline$wait_hlock_spec$ 
and \lstinline$pass_hlock_spec$), we now can define lock acquire and release 
functions for each shared memory operation. 
To define \lstinline$release_lock_AT_spec$, 
the release lock specification designated to the page allocation table, 
we simply combine \lstinline$release_shared$ and \lstinline$pass_hlock_spec$, and refine the general 
purpose shared memory event to the allocation table type event (\lstinline$OATE (a: ATable)$).}

\ignore{Then, using this lock in  \lstinline$palloc$ function and verifying the correctness of the code is simple. 
We only need to encapsulate the  \lstinline$palloc$ code with \lstinline$acquire_lock_AT$, 
the acquire lock specification for the page allocation table,
and \lstinline$release_lock_AT$ primitives to protect the operation on the allocation table. 
Thanks to the proofs that we have done in MCS Lock module, our \lstinline$palloc$ proof also contains the mutual exclusiveness, linearizability, liveness, and functional correctness.
Note that other client codes also use this approach, which makes us to easily re-use the verified MCS Lock for different kinds of client codes. }

\paragraph{Proof Effort}


Among the whole proofs, the most challenging parts are the proofs for starvation 
freedom theorems like Thm.~\ref{thm:mcs_wait_lock_exist}, 
and the functional correctness proofs for \lstinline$mcs_acquire$ 
and \lstinline$mcs_release$ functions
in Sec.~\ref{subsec:atomicoperation}.
The total lines of codes for starvation freedom is 2.5K lines, 0.6K lines for specifications, 
and 1.9k lines for proofs. This is because of the subtlety of those proofs. 
To prove the starvation freedom theorems and show the evidence of loop termination,
lots of lemmas are required to express
state changes by replaying the log. For instance, 
when \lstinline$QS_CalLock(l) = Some(c1, c2, b, q, s, t) $ 
and \lstinline$q = nil$, \lstinline$s$ $=\emptyset$ 
and \lstinline$t = nil$. It looks trivial in the hand-written proofs, 
but requires multiple lines of codes in the mechanized proof. 

The total lines of codes for the low-level functional correctness 
of \lstinline$mcs_acquire$ and \lstinline$mcs_release$ are 3.2K lines,  
0.7K lines for specifications, and 2.5K lines for proofs.
It is much bigger than other code correctness proofs for while-loops in CertiKOS, 
because these loops do not have any explicit decreasing value.
One another big part in our MCS Lock proofs is the proofs for 
Thm.~\ref{thm:machine-state-refinement} and the lines of code for this part is 
approximately 5K lines. The log replay function (\lstinline$CalMCSLock$) always 
return the whole MCS Lock values (\lstinline$MCSLock$) related 
to the  \lstinline$mcs_lock$ structure defined in Fig.~\ref{fig:exp:mcs_lock}. 
In this sense, we always have to give the exact values for all memory 
chunks and prove the correspondence between the memory and the abstract 
data even the event associated with reading values (e.g. \lstinline$GET_NEXT$).
Hence, those proofs contain a lot of duplicate proofs for the memory access. 
However, they are quite straightforward and easy to produce. 
On top of that, we strongly believe 
that they can be easily reduced by introducing mechanized user-defined tactics later. 

As an evaluation, we do not count the total lines of code in Coq for our entire 
MCS Lock module due to the two following reasons. First, our MCS Lock implementation 
is a part of CertiKOS. Therefore, our MCS Lock module also contains several definitions 
and proofs that are totally irrelevant to MCS Lock verification. 
This implies that counting the total lines of code for MCS Lock module has a 
high possibility of misinterpretation due to the lines of code for those definitions and proofs.
Second, we intensively use contextual refinement approach to 
build the whole system rather than focusing on verifying the correctness and 
liveness of MCS Lock. Therefore, our proof efforts are mainly focus on proving 
MCS Lock that is able to be easily combined with multiple client codes 
rather than the efficient lock verification itself.  

As can be seen from these line counts, proofs about concurrent programs
have a huge ratio of lines of proof to lines of C code.
If we tried to directly verify shared objects that use locks to 
perform more complex operations, like thread scheduling
and inter-process communication, a monolithic proof  
would become much bigger than the current one, and would be quite
unmanageable. The modular lock specification is essential here.


By contrast, the proofs for them in CertiKOS are quite tractable, 
because the proofs for the locks are modular, re-usable, and can 
be combined with other client-part proofs like we have briefly 
mentioned earlier in this Section.
Therefore, we believe that our approach is a promising way to 
show the correctness of large systems that use shared objects with mutex protection. 


The total lines of code in Coq for the MCS Lock module is approximately 30k lines, 9k lines for specifications and 21k lines for proofs.
Compared to the lines of C code in Section~\ref{sec:overview}, the verification effort is quite huge, although
those proofs also contain a lot of parts unrelated to the MCS Lock verification because of the primitives that are used in other modules of CertiKOS.

However, even if we exclude the proofs for those other modules and look at the proofs which are directly related to the MCS Lock code, the proof size is very large.
The proofs related to the two highest layers, which deal with designing atomic MCS Lock specifications, contain is approximately 8k lines of Coq including 2.5k lines of starvation freedom proofs.
Evidently, proving correctness of concurrent code is not easy work even for this short and seemingly simple program.

All the result in here implies that concurrent program proofs has a huge ratio in terms of the lines of code written in C versus the lines of code for the verification.
If we tried to directly verify shared objects that use the lock to  perform more complex operations, such as \texttt{thread scheduling} and \texttt{inter process communication (IPC)}, a monolithic proof would become much bigger than the current one, and would be quite unmanageable.
By contrast, the proofs for those features in CertiKOS are quite tractable, because the proofs for the locks are modular, re-usable, and can be combined with other client-part proofs. 
Therefore, we believe that our approach is a promising way to show the correctness of large systems that use shared objects with mutex protection, and using interactive theorem provers like Coq is desirable for this concurrent object proofs, even if the code is quite simple like this MCS algorithm. 

