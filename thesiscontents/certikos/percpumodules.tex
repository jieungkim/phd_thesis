\section{Per-CPU Modules}
\label{chapter:certikos:sec:per-cpu-modules}

Per-CPU modules in $\certikos$ (Figure~\ref{fig:chapter:certikos:top-level-multicore-theorem}) is refining a part of $\certikos$ modules,
which are included in $\certikos_{\codeinmath{cpu}}$ in the figure, based on the $\codeinmath{MBoot}$ layer, 
into the local layer interface, $\codeinmath{PBThrd}$.
Those modules consist of six components;
two spinlock modules, a device driver module, a memory management module,  a process (thread) management module, and a virtual machine management module. 


\subsection{Spinlock Module}
\label{chapter:certikos:subsec:spinlock-module}

Spinlock provides the protection mechanism for shared resources by using its safety property, mutual exclusion of the owner for each lock. 
We build two spinlock modules, 
MCS Lock~\cite{mcs91} and Ticket Lock~\cite{lwn:ticketlocks} modules.
Since we have already discussed MCS Lock in Chapter~\ref{chapter:mcs-lock}, 
we only explain ticket lock here.

The concept of a ticket lock is similar to the ticket management system that is common in many bakeries, banks, and delis. 
In the system, 
there is a dispenser which gives the numbered ticket to the clients upon arrival. 
Each time the next ticket number is ready to 
be served, 
the system updates now serving sign by increasing the now-serving number. 
\begin{figure}
\lstinputlisting[language = C, multicols=2]{source_code/certikos/ticketlock.c} 
\caption{Ticket Lock Implementation (Written in C.)}
\label{fig:chapter:certikos:ticket-lock-example}
\end{figure}
Implementing a ticket lock follows the same manner with this high-level idea.
It is built on top of an atomic object that contains two fields, \textit{next} and \textit{now},
and operations for a dispenser, checking the now-serving number, as well as increasing the now serving number when
it is ready to serve the next ticket. 
Figure~\ref{fig:chapter:certikos:ticket-lock-example} shows the implementation of our ticket lock (few insignificant parts are omitted.)
Our lock module provides multiple lock instances associated with their own lock identifier;
thus all operations on the lock object specify the lock ID as its argument, which is $lkid$ in this example.
We provides three getter/setter primitives, $\codeinmath{FAI}$, $\codeinmath{get\_now}$, and $\codeinmath{incr\_now}$,
that correspond to the three operations of the atomic object.
Among them, the atomic increment to the ticket (the operation of the dispenser) 
uses the atomic fetch-and-increment (FAI) operation, that is implemented using the $\codeinmath{xaddl}$ instruction with the 
lock prefix in $\intelmachine$.

We also prove the starvation freedom property for this Ticket Lock based on the assumption about hardware fairness,
which implies that each CPU has a fair chance to execute its one program instance.
The starvation freedom proof in this ticket lock is 
similar to that of MCS Lock in Chapter~\ref{chapter:mcs-lock}, but 
it's simpler than that due to the simplicity of it.

\subsection{Device Driver Module}
\label{chapter:certikos:subsec:device-driver-module}

$\certikos$ supports device driver handling, and all these works are from the previous work by  Chen \etal~\cite{certikos:interrupt}.
The work introduces a device model for multiple hardware devices,
as well as a formal model for interrupts. 
Based on them, they verify interruptible code and insert them into the sequential $\certikos$~\cite{deepspec}, a prior work of our concurrent $\certikos$. 
The work has  successfully ported to the concurrent $\certikos$
by clearly separating the concurrent aspects of multicore/multithreaded environments
with those triggered by devices.
$\certikos$ has a single designated thread that is responsible for those interrupts,
and our future direction is extending the current version to support more efficient concurrent-handling model that can cover both device drivers and multicore/multithreaded environments together.

\subsection{Memory Management Module}
\label{chapter:certikos:subsec:memory-management-module}

We built a memory management module with 15 layers, 
which supports container for resource management, 2-level page tables, and dynamic page allocations for users.
This module also contains the shared memory library,
but we omit to explain the library because our thread linking does not include that. 

\subsubsection{Container} 
% container
\begin{figure}
\begin{minipage}[t]{.45\textwidth}
\raggedright
\lstinputlisting[language = C]{source_code/certikos/container.c} 
\begin{center}
(a) Data Type Def. for Container in C
\end{center}
\end{minipage}
\hfill
\noindent
\begin{minipage}[t]{.45\textwidth}
\raggedleft
\lstinputlisting[language = C]{source_code/certikos/container.v} 
\begin{center}
(b) Data Type Def. for Container in $\coq$
\end{center}
\end{minipage}
\caption{Container Data Type Definition (Written in C and $\coq$.)}
\label{fig:chapter:certikos:container}
\end{figure}
The container manages the resource that each process (its children) can facilitate; 
In the case of memory management, for instance,
the container memorizes the maximum number of pages (via the process' quota) 
that each process can use or hand it over to its children.
Definitions of container data structure (both in C and $\coq$) are in 
Figure~\ref{fig:chapter:certikos:container}. 
By following our \textit{fun-lift} pattern of $\ccalname$ (Section~\ref{chapter:ccal:subsec:local-layer-interface}, 
we built getter/setter functions based on the C data structure (left in the figure), 
and refines them with the functions that uses $\coq$ data structure (right in the figure.)

Our container object stems from the notion of containers and quotas in HiStar~\cite{zeldovich06};
thus our process creation is parameterized by the number for the process' quota, the maximum number of pages that 
the process can occupy.
This Quota works as a protection for a denial-of-service attack, where one process repeatedly allocates pages and uses up all available memory. 
Denying other threads from allocating pages via a denial-of-service attack due to this quota system. 
Since each process has its capacity, all processes can fairly use memory pages even though some processes occupy their maximum pages specified in quota. 
It also prevents the undesirable information channel between different processes that occur due to such an attack.

This container is a tree structure, and each CPU contains their tree with the root process for each core. 
With this separation, the container of each core does not interfere by other containers in other cores. 

\subsubsection{Physical memory management} 
% physical memory management
\begin{figure}
\lstinputlisting[language = C, multicols=2]{source_code/certikos/palloc.c} 
\caption{Page Allocation Function Implementation (Written in C.)}
\label{fig:chapter:certikos:palloc-in-c}
\end{figure}
Our physical memory management module builds a physical page allocation map as well as a dynamic page allocator $\pallocfunc$, briefly explained 
in Section~\ref{chapter:mcslock:sec:evaluation} with the source code in Figure~\ref{fig:chapter:mcslock:palloc-example}.
As discussed in the section, the page allocation table $\codeinmath{AT}$ is shared by 
all CPUs in the system; thus
the designated lock, $\codeinmath{acquire\_lock\_AT}$ and  $\codeinmath{release\_lock\_AT}$,
encapsulates the function $(\codeinmath{palloc\_aux}$ that touches and manipulate the $\codeinmath{AT}$.
The full source code for both functions are in Figure~\ref{fig:chapter:certikos:palloc-in-c}.
Once we prove that the correctness of those functions, we facilitate 
the \textit{log-lift} pattern of $\ccalname$ (Section~\ref{chapter:ccal:subsec:local-layer-interface}) to simplify the interleaving for this object,
and introduce 
an atomic object for this page allocation table, by merging the lock acquire and lock release as well as 
the event for the shared resources together (See Section~\ref{chapter:mcslock:sec:evaluation} for more details.)
\begin{figure}
\lstinputlisting[language = C]{source_code/certikos/palloc_spec.v} 
\caption{Page Allocation Primitive Specification (Written in $\coq$.)}
\label{fig:chapter:certikos:palloc-in-coq}
\end{figure}
The high-level specification for $\pallocfunc$ is in Figure~\ref{fig:chapter:certikos:palloc-in-coq}.
In the figure, \lstinline$RData$ is a type of abstract data, $AC$ is a container field in the abstract data. 
\lstinline$CalAT_log_real$ is the function to pull out the current allocation table state by replaying  the log
related to page allocations in the system, and \lstinline$pperm$ is a CPU-local page permission table. 
With that given information, \lstinline$palloc_spec$ updates a single event (\lstinline$OPALLOCE$) into the log based on the result of the page allocation operation with considering the relate interleaving with other CPUs via the environmental context (\lstinline$multi_oracle$ in the figure.)
The lock instance for this page allocation, one element in our spinlock pool, are not allowed to be invoked at higher layers;
thus processes that hold the lock for other shared objects 
cannot hold (and does not release) the lock for page allocation.
With providing this atomic interface for those locks,
processes precisely follow the order of acquiring and releasing locks with the layer order, which implies that 
the lock in the higher layer will be acquired before the lock in the lower layer,
and the lock in the lower layer will be released before the lock in the higher layer.
This implicit order of lock acquisitions prevents deadlocks in the $\certikos$ kernel.


\subsubsection{Virtual memory management}
The virtual memory management module is built on top of the physical memory management layers. 
Our virtual memory management is
based on the explicit modeling of the memory management unit (MMU) in our abstract data and thus abstract layers. 
Our MMU model follows and mirrors the paging hardware; thus memory accesses made by both the kernel and the user programs are translated using the two-level page maps. 
The purpose of our verification in the virtual memory management module is
not only showing the correctness of the manipulation on the page maps but also proving 
that the initialization procedure sets up the two-level page maps correctly regarding the hardware address translation.

\begin{figure}
\lstinputlisting[language = C]{source_code/certikos/pagetable.v} 
\caption{Page Table Abstract Data Structure Definition (Written in $\coq$.)}
\label{fig:chapter:certikos:page-table}
\end{figure}
In detail, the page table is a partial map from a process ID to its page table as the definition related to the page table (written in $\coq$) is in
Figure~\ref{fig:chapter:certikos:page-table}. 
Page table 0 is reserved for the kernel page table, which is defined as an identity map,
and we map the low memory as an identity map with kernel only permission
for every process' page table.
It reduces the complexity of our permission resolution because when each process traps into the kernel,
the kernel can still access kernel data natively via memory access
without switching to the kernel page table. 
In the figure, 
\lstinline$IDPDE$ is an identity  map, which maps its address to every page directory entry when we need an identity mapping.
The page table structure, \lstinline$PMapPool$, is a partial map from a process identifier to its 2-level page table , \lstinline$PMap$.
 Each page directory entry (\lstinline$PDE$) has one of four values;
 1) the proper page table entry (\lstinline$PDEValid$); 2) an identity map (\lstinline$PDEID$); 3) unallocated (\lstinline$PDEUnPresent$); or 4) uninitialized or invalid (\lstinline$PDEUndef$). 
A page table entry is one of three values;
1)  a valid address with proper permission (\lstinline$PTEValid$); 2) remaining unmapped (\lstinline$PTEUnPresent$);
 or 3) uninitialized or invalid (\lstinline$PTEUndef$).

 \begin{figure}
\lstinputlisting[language = C]{source_code/certikos/ptresv_spec.v} 
\caption{Pate Table Reservation Primitive Specification (Written in $\coq$.)}
\label{fig:chapter:certikos:ptresv-func}
\end{figure}
 One important function that manipulates the page table is a page table reservation function,
 which updates each process' page table.
 The specification for the function is defined in Figure~\ref{fig:chapter:certikos:ptresv-func},
  where  $\codeinmath{ptInsert\_spec}$ is a specification for the $\codeinmath{ptInsert}$ function which links 
the physical address mapping with the virtual address mapping. 
The function first checks whether the corresponding page directory entry is already mapped to any page table entries.
If it is not, it allocates one by invoking $\codeinmath{ptAllocPDE}$ primitive that corresponds to the  $\codeinmath{ptAllocPDE\_spec}$
in the figure.
The $\codeinmath{ptInsertPTE\_spec}$ is the specification for function $\codeinmath{ptInsertPTE}$ that actually inserts the page 
information into the page table. 
When either of any processes fails during this evaluation, it returns the magic number as its result, which is distinguishable with 
any return values of the function.

\subsection{Intel Virtualization Module}
\label{chapter:certikos:subsec:virtualization-module}

Our $\certikos$ can run at most one virtual machine per one core;
and our Intel virtualization module implements layers related to the virtual machine management,
the hardware-assisted virtualization technology Intel VT-x.  
The module introduces the four-level Extended Page Table (EPT), 
similar to the two-level in-kernel page table structure except having  two additional levels, 
as well as several primitives to manipulate the extended page entries.
It also contains layers to introduce the Virtual Machine Control Structure (VMCS) and the
 Virtual Machine Extension metadata (VMX).
Those layers introduce VMCS and VMX objects, which consists of their data structures as well as primitives. 
They model VMCS and VMX data structures  in their 
 abstract layers (introduce those data structures as parts of an abstract state)
as well as provide primitives that properly initialize and manipulate them. 
This module also introduces two $\codeinmath{vm\_enter}$ and $\codeinmath{vm\_exit}$ primitives to enter and exit from the host mode.

\jieung{Need to change the word process (or thread) to make them to be consistent}
\subsection{Process Management Module}
\label{chapter:certikos:subsec:process-management-module}
Our process management module contains 
a kernel context switching, an abstract queue library, thread management which includes scheduling primitives (\ie, yield, sleep, wake up)
and dynamic process creation. 

\subsubsection{Kernel Context Switching}

\begin{figure}
\lstinputlisting[language = C,multicols=2]{source_code/certikos/kctxt_switch.S} 
\caption{Kernel Context Switch Implementation (Written in Assembly.)}
\label{fig:chapter:certikos:kernel-context-switch-impl}
\end{figure}
Our process management first introduces the context switching written in Assembly, of which the implementation is in Figure~\ref{fig:chapter:certikos:kernel-context-switch-impl}.
We first abstracts the kernel context pool in the memory ($\codeinmath{KCtxtPool\_LOC}$ in Figure~\ref{fig:chapter:certikos:kernel-context-switch-impl}) 
into the kernel context map in the abstract data:
\begin{lstlisting}[language = C]
Definition KContext := regset.
Definition KContextPool := ZMap.t KContext.
\end{lstlisting}
By using that, we provide the layer interface that contains the kernel context switching as its primitive
\lstinputlisting[language = C]{source_code/certikos/kctxt_switch.v}
which stores the kernel context of $\codeinmath{src}$ and returns the kernel context of $\codeinmath{des}$ as its result.


\jieung{I need to decide whether I will add this subsubsection or not}
\subsubsection{User Context Switching}

A few layers introduce the user context that contains the list of user registers that we save and restore when 
the context switching happens between users and kernel (via trap and system calls.)
Similar to the case of kernel context switching, $\certikos$ models the user context in the abstract data and verify the functionality of user context switching. 
It, however, differs from the kernel context switching.
User context switching specifications in $\certikos$ only describes the saves and restores of the second half of the full context switching.
The hardware has responsibilities for the first half, which will automatically save and restore them.


\subsubsection{Abstract Queue Library}

The abstract queue with having a FIFO property works as bases of multiple crucial services of $\certikos$. 
The thread management and the condition variable (via sleep and wake-up) in our kernel heavily rely on this abstract queue library. 
The implementation of this abstract queue uses 
doubly-linked lists; thus each node in lists is defined as 
\begin{lstlisting}[language = C, multicols=2]
// TDQ in C
struct TDQ_node{
uint head; uint tail; };
// TDQ in Coq
Inductive TDQueue :=
| TDQUndef | TDQValid (head tail: Z).   
\end{lstlisting}
To provide the simple interface for the queue, 
we abstracts the queue state as $\coq$ lists as
\begin{lstlisting}[language = C]
Inductive AbQueue := | AbQUndef | AbQValid (l: list Z).
\end{lstlisting}
and local queue operations,
 \textit{enqueue} and \textit{dequeue},
are directly specified as the operations over the abstract lists as follows:
\begin{lstlisting}[language = C]
Function local_enqueue_spec (n i: Z) (adt: RData): option RData :=
  <@$\cdots$@>
  {abq: ZMap.set n (AbQValid (l ++ (i::nil))) (abq adt)}
  <@$\cdots$@>
\end{lstlisting}
Since those queues can be shared by multiple processes in the system, 
we associate each shared queue with a lock,
\begin{lstlisting}[language = C]
Function enqueue_atomic_spec (n i: Z) (adt: RData): option RData :=
  match acquire_lock_ABTCB_spec n adt with
  | Some adt0 => match local_enqueue_spec n i adt0 with
                             | Some adt1 => release_lock_ABTCB_spec n adt1
                             | _ => None
                             end
  | _ => None
  end.
\end{lstlisting}
and provide the atomic interface for the queue by using the \textit{log-lift} pattern in $\ccalname$.


\subsubsection{Thread Management}
The thread management module introduces the  thread control block, 
that contains the status of each thread (\ie, ready, run, \etc)
as well as other information for the thread including the information about its quotas, parent, and children.

The thread scheduler is associated with thread primitives; yield, sleep, and wake-up, which uses the kernel context switching and the abstract queue library. 
The ready
Each CPU has a private ready queue and a shared pending queue (for our condition variable.) 
The ready queue is private to each CPU; thus operations on the ready queue is only allowed by the owner of the queue.
On the other hand, other CPUs in the system can insert threads to the current CPU’s pending queue. 
$\certikos$ kernel also provides a set of sleeping queues, which are shared among all CPUs.
Each sleeping queue is associated with its condition variable. 

The \textit{yield} primitive works with three operations; 
(1) when the pending queue is not empty, dequeue the head of its pending queue and add it to its ready queue;
(2) enqueue the running thread to the ready queue; and 
(3) dequeue the head of its ready queue to make it run.

The \textit{sleep} primitive has two steps;
(1) enqueue the running thread to the shared sleeping queue associated with the condition variable, which is an argument of the sleep primitive; and
(2) dequeue the head of its ready queue to make it run.

The \textit{wake-up} primitive contains two cases. 
The first is when the thread that wants to wake up is the thread on the current CPU. 
In this case, the primitive adds the thread to its ready queue because the primitive has permission to manipulate the ready queue directly.
When the thread that will be wakened up is not in the current CPU, however, the wake-up primitive adds the thread to the pending queue of the CPU
that the thread belongs to (\ie, notes that all CPUs can manipulate other CPUs' pending queues.) 

\jieung{I found that we cannot guarantee liveness of the thread in the pending queue when proc create is infinitely invoked because it does not
check the pending queue. But, it should be ok because our proc create is bounded with the maximum number of threads for each CPU.}
\jieung{Can I use the scheduling figure in certikos paper (Figure 9)? Even if I can use that one, I need to modify it by adding proc create.}

The process creation primitive also affects the scheduling.
When one process wants to create its child, 
it first checks the quota that the child wants to have (\ie, it is the argument of the primitive), 
and then create the child by giving the new process ID to the child. 
It then adds the newly created child at the end of the ready queue of the CPU that the parent and the newly created child belong to it. 
As seen in these scheduling procedures, we do not allow the process migration from one CPU to others.

Like what our layers do for other shared primitives, $\certikos$ provides the layer interface ($\codeinmath{PBThrd}$)
that contains atomic specifications for all scheduling related primitives. 
The layer, $\codeinmath{PBThrd}$, also merges all separate logs\footnote{Note that we divide the global log into multiple individual logs is associated with the corresponding shared objects from $\codeinmath{MBoot}$. The division is for the simplicity of our layer building, but we have to merge them into the single global log for multithreaded linking. We mentioned about it in multiple places in Chapter~\ref{chapter:mcs-lock} and Chapter~\ref{chapter:linking}} 
into a single log to provide the top-most layer of our per-CPU modules (and thus the base layer for our multithreaded linking.)
Figure~\ref{fig:chapter:certikos:proc-create-cpu-spec} and Figure~\ref{fig:chapter:certikos:thread-yield-cpu-spec} show the specification of the process creation primitive and the specification of the thread yield primitive, respectively.

\begin{figure}
\lstinputlisting[language = C]{source_code/certikos/cpu_proc_create.v} 
\caption{Process Create Primitive Specification in $\codeinmath{PBThrd}$ (Written in $\coq$.)}
\label{fig:chapter:certikos:proc-create-cpu-spec}
\end{figure}


\begin{figure}
\lstinputlisting[language = C]{source_code/certikos/cpu_thread_yield.v} 
\caption{Thread Yield Primitive Specification in $\codeinmath{PBThrd}$ (Written in $\coq$.)}
\label{fig:chapter:certikos:thread-yield-cpu-spec} 
\end{figure}


\subsection{Connecting All per-CPU layers}
\label{chapter:certikos:subsec:connecting-all-per-cpu-layers}

We have introduces 60 layers for all those modules in our per-CPU layers, from the $\codeinmath{MBoot}$ layer to the 
$\codeinmath{PBThrd}$ layer.
All those layers can be linked together via the composition rules described in Section~\ref{chapter:ccal:subsec:linking};
thus form a single contextual refinement theorem, which is defined in Theorem~\ref{theorem:chapter:certikos:per-cpu-contextual-refinement},
which corresponds to the number (1) in Figure~\ref{fig:chapter:certikos:top-level-multicore-theorem}.
\begin{theorem}[Contextual Refinement for Per-CPU layers]
\label{theorem:chapter:certikos:per-cpu-contextual-refinement}
Let's assume that $cid$ is a valid CPU ID (in between 0 and 7), $\oracle_{\codeinmath{cpu}}$ and  $\oracle'_{\codeinmath{cpu}}$ are 
environmental contexts for $\codeinmath{MBoot}$  and $\codeinmath{PBThrd}$ layers, respectively, and $\codeinmath{CTXT}$ is an any context program. Then, we can say that
\begin{center}
$\semwmachine{\codeinmath{MBoot}[cid, \oracle_{\codeinmath{cpu}}]}{\certikos_{\codeinmath{cpu}} \oplus \codeinmath{Ctxt}}{\codeinmath{mach}_{\lasm}} \refines_{R_{\certikos_\codeinmath{cpu}}} \semwmachine{\codeinmath{PBThrd}[cid, \oracle'_{\codeinmath{cpu}}]}{\codeinmath{Ctxt}}{{\codeinmath{mach}_{\lasm}}}$
\end{center}
when ${R_{\certikos_\codeinmath{cpu}}}$ is a refinement relation between those two layers (the refinement relation can be achieved by 
connecting all the refinement relations in between 60 per-CPU layers.)
\end{theorem}

%Informally, this theorem states the following property:
%\begin{quote} 
%"any context program $\codeinmath{Ctxt}$ running on the $\codeinmath{PBThrd}$ layer (the top-most layer interface of per-CPU layers) contextually refines the context program $\codeinmath{Ctxt}$ plus  the implementation of CPU local modules of $\certikos$, $\certikos_{\codeinmath{cpu}}$,
%running on $\codeinmath{MBoot}$ (the bottom-most layer interface of per-thread layers) with LAsm".
%\end{quote}
%%

%This corresponds to the number (14) in Figure~\ref{fig:chapter:certikos:top-level-multicore-theorem}.

%
%\begin{lstlisting}[language=C]
%  Theorem Per_CPU_CertiKOS_correct:
%    forall (s: stencil) (CTXT: LAsm.module) kernel combined_program user_program
%           (builtin_idents_norepet_prf: CompCertBuiltins.BuiltinIdentsNorepet),
%      CertiKOS.per_cpu_certikos = OK kernel ->
%      make_program s (CTXT <@$\oplus$@> kernel) (MBoot.mboot <@$\oplus$@> L64) = OK combined_program ->
%      make_program s CTXT (PBThread.pbthread <@$\oplus$@> L64) = OK user_program ->
%      inhabited
%        (backward_simulation
%           (LAsm.semantics (lcfg_ops := LC (PBThread.pbthread <@$\oplus$@> L64)) user_program)
%           (LAsm.semantics (lcfg_ops := LC (MBoot.mboot <@$\oplus$@> L64)) combined_program)).
%\end{lstlisting}
