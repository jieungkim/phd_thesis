\section{Per-CPU Modules}
\label{chapter:certikos:sec:per-cpu-modules}

Per-CPU modules in $\certikos$  refines related parts of  $\certikos$ modules--which are included in $\certikos_{\codeinmath{cpu}}$ in the figure, based on the $\mmcsbootfull$ layer-- 
into the local layer interface, $\PBThrd$.
Those modules consist of six components:
two spinlock modules, a device driver module, a memory management module,  a thread-management module, and a VM management module. 
\jieung{process? thread? which one will I choose? --- thread}

\subsection{Spinlock Module}
\label{chapter:certikos:subsec:spinlock-module}

Spinlock provides the protection mechanism for shared resources by using its safety property--mutual exclusion of the owner for each lock. 
We build two spinlock modules, 
MCS Lock~\cite{mcs91} and Ticket Lock~\cite{lwn:ticketlocks}.
Because we have already discussed MCS Lock in Chapter~\ref{chapter:mcs-lock}, 
we only explain ticket lock here.

The concept of a ticket lock is similar to the ticket-management system that is common in many bakeries, banks, and delis. 
In the system, 
there is a dispenser which gives the numbered ticket to the clients upon arrival. 
Each time the next ticket number is ready to 
be served, 
the system updates the now-serving sign by increasing the now-serving number. 
\begin{figure}
\lstinputlisting[language = C, multicols=2]{source_code/certikos/ticketlock.c} 
\caption{Ticket Lock Implementation (Written in C).}
\label{fig:chapter:certikos:ticket-lock-example}
\end{figure}
Implementing a ticket lock follows the same manner with this high-level idea.
It is built on top of an atomic object that contains two fields--\textit{next} and \textit{now}--and operations for a dispenser, checking the now-serving number and increasing the now-serving number when
it is ready to serve the next ticket. 
Figure~\ref{fig:chapter:certikos:ticket-lock-example} shows the implementation of our ticket lock (a few insignificant parts are omitted.)
Our lock module provides multiple lock instances associated with their own lock identifiers;
thus, all operations on shared objects specify their lock IDs as their argument, which is $lkid$ in this example.
We provide three getter/setter primitives-- $\codeinmath{FAI}$, $\codeinmath{get\_now}$, and $\codeinmath{incr\_now}$--
that correspond to the three operations of the atomic object.
Among them, the atomic increment to the ticket (the operation of the dispenser) 
use the atomic fetch-and-increment (FAI) operation that is implemented using the $\codeinmath{xaddl}$ instruction with the 
lock prefix in $\intelmachine$.

We also prove one progress property--starvation freedom--for this Ticket Lock based on the assumption about hardware fairness,
which implies that each CPU has a fair chance to execute its one program instance.
The starvation freedom proof in this ticket lock is 
similar to that of the MCS Lock in Chapter~\ref{chapter:mcs-lock}, but 
it's much simpler than the proof of the MCS Lock.

\subsection{Device Driver Module}
\label{chapter:certikos:subsec:device-driver-module}

$\certikos$ supports device-driver handling, and all these works are from the previous work by  Chen \etal~\cite{certikos:interrupt}.
The work introduces a device model for multiple hardware devices,
as well as a formal model for interrupts. 
Based on them, we verify interruptible codes and insert them into the sequential $\certikos$~\cite{deepspec}, a prior work of our concurrent $\certikos$. 
The work has  successfully ported to the concurrent $\certikos$
by clearly separating  concurrent aspects of multicore/multithreaded environments
with those triggered by devices.
$\certikos$ has a single designated thread that is responsible for those interrupts,
and our future direction is to extend the current version to support a more efficient concurrent-handling model that can cover both device drivers and multicore/multithreaded environments together.

\subsection{Memory-Management Module}
\label{chapter:certikos:subsec:memory-management-module}

We built a memory-management module with 15 layers, 
which supports a container for resource management, a two-level page table, and a dynamic page allocation for users.
This module also contains a shared memory library,
but we omit an explanation of the library because our thread linking does not include that. 

\subsubsection{Container} 
% container
\begin{figure}
\begin{minipage}[t]{.45\textwidth}
\raggedright
\lstinputlisting[language = C]{source_code/certikos/container.c} 
\begin{center}
(a) Data Type Def. for Container in C
\end{center}
\end{minipage}
\hfill
\noindent
\begin{minipage}[t]{.45\textwidth}
\raggedleft
\lstinputlisting[language = C]{source_code/certikos/container.v} 
\begin{center}
(b) Data Type Def. for Container in $\coq$
\end{center}
\end{minipage}
\caption{Container Data Type Definition (Written in C and $\coq$).}
\label{fig:chapter:certikos:container}
\end{figure}
A container manages the resource that each thread (its children) can facilitate.
In the case of memory management, for instance,
a container memorizes the maximum number of pages (via the thread's quota) 
that each thread can use or hand it over to its children.
Definitions of our container data structure (both in C and $\coq$) are in 
Figure~\ref{fig:chapter:certikos:container}. 
By following our \textit{fun-lift} pattern of $\ccalname$ (Section~\ref{chapter:ccal:subsec:local-layer-interface}, 
we built getter/setter functions based on this C data structure (left in the figure), 
and refine those getter/setter functions with the functions that use our $\coq$ data structure (right in the figure.)


Our container object stems from the notion of containers and quotas in HiStar~\cite{zeldovich06};
thus, our thread spawn is parameterized by the number for the thread's  quota--the maximum number of pages that 
the thread can occupy.
This quota works as a protection against a denial-of-service attack, where one thread repeatedly allocates pages and uses up all available memory
by denying other threads from allocating pages.
Since each thread has its capacity, all threads can fairly use memory pages even though some threads occupy their maximum pages specified in quota. 
This also prevents the undesirable information channel between different threads that occur due to such an attack.

This container is a tree structure, and each CPU contains its tree with the root thread for each core. 
With this separation, the container of each core does not interfere with other containers in other cores. 

\subsubsection{Physical memory management} 
% physical memory management
\begin{figure}
\lstinputlisting[language = C, multicols=2]{source_code/certikos/palloc.c} 
\caption{Page Allocation Function Implementation (Written in C).}
\label{fig:chapter:certikos:palloc-in-c}
\end{figure}
Our physical memory-management module builds a physical page-allocation map as well as a dynamic page allocator $\pallocfunc$, briefly explained 
in Section~\ref{chapter:mcslock:sec:evaluation} with the source code in Figure~\ref{fig:chapter:mcslock:palloc-example}.
As discussed in the section, the page-allocation table $\codeinmath{AT}$ is shared by 
all CPUs in the system; so
the designated lock--$\codeinmath{acquire\_lock\_AT}$ and  $\codeinmath{release\_lock\_AT}$--encapsulates the function $\codeinmath{palloc\_aux}$ that touches and manipulates the $\codeinmath{AT}$.
The full source codes for both functions are in Figure~\ref{fig:chapter:certikos:palloc-in-c}.
Once we prove  the correctness of those functions, we facilitate 
our \textit{log-lift} pattern to simplify  interleavings for this object,
and we introduce 
an atomic object for this page allocation table, by merging  lock acquire and lock release events as well as 
the event for the shared resources  (see Section~\ref{chapter:mcslock:sec:evaluation} for more details).
\begin{figure}
\lstinputlisting[language = C]{source_code/certikos/palloc_spec.v} 
\caption{Page-Allocation Primitive Specification (Written in $\coq$).}
\label{fig:chapter:certikos:palloc-in-coq}
\end{figure}
The high-level specification for $\pallocfunc$ is in Figure~\ref{fig:chapter:certikos:palloc-in-coq}.
In the figure, \lstinline$RData$ is a type of abstract data, and $AC$ is a container field in the abstract state.
\lstinline$CalAT_log_real$ is a function to pull out the current allocation table state by replaying the log
related to page allocations in the system, and \lstinline$pperm$ is a CPU-local page permission table. 
With that given information, \lstinline$palloc_spec$ updates a single event (\lstinline$OPALLOCE$) into the log, based on the result of the page-allocation operation and considering the related interleaving with other CPUs via the environmental context (\lstinline$multi_oracle$ in the figure.)
The lock instance for this page allocation--one element in our spinlock pool--is not allowed to be invoked at higher layers;
thus, threads that hold the lock for other shared objects cannot hold (and does not release) the lock for page allocation.
By providing this atomic interface for those locks,
threads precisely follow the order of acquiring and releasing locks with the layer order, which implies that 
the lock in the higher layer will be acquired before the lock in the lower layer,
and the lock in the lower layer will be released before the lock in the higher layer.
This implicit order of lock acquisitions prevents deadlocks in the $\certikos$ kernel.



\subsubsection{Virtual memory management}
The virtual memory-management module is built on top of  physical memory-management layers. 
Our virtual memory-management is
based on the explicit modeling of the memory management unit (MMU) in our abstract data and thus on abstract layers. 
Our MMU model follows and mirrors the paging hardware; thus, memory accesses made by both the kernel and the user programs are translated using the two-level page maps. 
The purpose of our verification in the virtual memory-management module is
not only to show the correctness of the manipulation on the page maps but also to provide 
that the initialization procedure sets up the two-level page maps correctly regarding the hardware address translation.

\begin{figure}
\lstinputlisting[language = C]{source_code/certikos/pagetable.v} 
\caption{Page Table Abstract Data Structure Definition (Written in $\coq$).}
\label{fig:chapter:certikos:page-table}
\end{figure}
In detail, our page table is a partial map from a thread ID to its page table, as the definition related to the page table (written in $\coq$) in
Figure~\ref{fig:chapter:certikos:page-table}. 
The page table 0 is reserved for the kernel page table, which is defined as an identity map,
and we map the low memory as an identity map with kernel-only permission
for every thread's page table.
It reduces the complexity of our permission resolution because when each thread is trapped into the kernel,
the kernel can still access kernel data natively via memory access
without switching to the kernel page table. 
In the figure, 
\lstinline$IDPDE$ is an identity map, which maps its address to every page-directory entry when we need an identity mapping.
The page table structure, \lstinline$PMapPool$, is a partial map from a thread identifier to its two-level page table, \lstinline$PMap$.
 Each page directory entry (\lstinline$PDE$) has one of four values:
 1) the proper page table entry (\lstinline$PDEValid$); 2) an identity map (\lstinline$PDEID$); 3) unallocated (\lstinline$PDEUnPresent$); or 4) uninitialized or invalid (\lstinline$PDEUndef$). 
A page table entry is one of three values:
1)  a valid address with proper permission (\lstinline$PTEValid$); 2) remaining unmapped (\lstinline$PTEUnPresent$);
 or 3) uninitialized or invalid (\lstinline$PTEUndef$).

 \begin{figure}
\lstinputlisting[language = C]{source_code/certikos/ptresv_spec.v} 
\caption{Pate Table Reservation Primitive Specification (Written in $\coq$.)}
\label{fig:chapter:certikos:ptresv-func}
\end{figure}
 One important function that manipulates this page table is a page-table reservation function,
 which updates each thread's page table.
 The specification for the function is defined in Figure~\ref{fig:chapter:certikos:ptresv-func},
  where  $\codeinmath{ptInsert\_spec}$ is a specification for  $\codeinmath{ptInsert}$  which links 
a physical address mapping with a virtual address mapping. 
The function first checks whether the corresponding page directory entry is already mapped to any page-table entries.
If it is not, it allocates one by invoking $\codeinmath{ptAllocPDE}$ primitive that corresponds to the  $\codeinmath{ptAllocPDE\_spec}$
in the figure.
The $\codeinmath{ptInsertPTE\_spec}$ is the specification for  $\codeinmath{ptInsertPTE}$ that actually inserts the page 
information into the page table. 
When any threads fail during this evaluation, it returns the magic number as its result, which is distinguishable from 
any return values of the function.

\subsection{Intel Virtualization Module}
\label{chapter:certikos:subsec:virtualization-module}

Our $\certikos$ can run at most one virtual machine per  core:
and our Intel virtualization module implements layers related to the virtual machine management--
the hardware-assisted virtualization technology Intel VT-x.  
The module introduces a four-level Extended Page Table (EPT), 
similar to our two-level in-kernel page-table structure but  with two additional levels, 
as well as several primitives to manipulate the extended page entries.
It also contains layers to introduce the Virtual Machine Control Structure (VMCS) and the
 Virtual Machine Extension metadata (VMX).
Those layers introduce VMCS and VMX objects, which consists of their data structures as well as primitives. 
They model VMCS and VMX data structures  in their 
 abstract layers (introduce those data structures as parts of an abstract state)
as well as provide primitives that properly initialize and manipulate them. 
This module also introduces two $\codeinmath{vm\_enter}$ and $\codeinmath{vm\_exit}$ primitives to enter and exit from the host mode.

\jieung{Need to change the word process (or thread) to make them to be consistent}
\subsection{Thread-Management Module}
\label{chapter:certikos:subsec:process-management-module}
Our thread-management module contains 
a kernel context switching, an abstract queue library, thread management which includes scheduling primitives (\ie, yield, sleep, and wake up),
and dynamic thread spawn.

\subsubsection{Kernel Context Switching}

\begin{figure}
\lstinputlisting[language = C,multicols=2]{source_code/certikos/kctxt_switch.S} 
\caption{Kernel Context Switch Implementation (Written in Assembly).}
\label{fig:chapter:certikos:kernel-context-switch-impl}
\end{figure}
Our thread management first introduces a context switching written in Assembly, of which the implementation is in Figure~\ref{fig:chapter:certikos:kernel-context-switch-impl}.
We first abstract a kernel context pool in a memory ($\codeinmath{KCtxtPool\_LOC}$ in Figure~\ref{fig:chapter:certikos:kernel-context-switch-impl}) 
into a kernel context map in abstract data:
\begin{lstlisting}[language = C, deletekeywords = {Context}]
Definition KCtxt := regset.
Definition KCtxtPool := ZMap.t KCtxt.
\end{lstlisting}
By using that, we provide a layer interface that contains a kernel context switching as its primitive,
\lstinputlisting[language = C, deletekeywords = Context]{source_code/certikos/kctxt_switch.v}
which stores the kernel context of $\codeinmath{src}$ and returns the kernel context of $\codeinmath{des}$ as its result.


\jieung{I need to decide whether I will add this subsubsection or not}
\subsubsection{User Context Switching}

A few layers introduce a user context that contains the list of user registers that we save and restore when 
a context switching happens between users and the kernel (via trap and system calls.)
Similar to the case of a kernel context switching, $\certikos$ models a user context in abstract data and verifies the functionality of this user context switching. 
However, it differs from a kernel context switching.
User context switching specifications in $\certikos$ only describes the saves and restores of the second half of the full context switching.
The hardware has responsibilities for the first half, which will automatically save and restore them.



\subsubsection{Abstract Queue Library}

An abstract queue with having a FIFO property works as bases of multiple crucial services of $\certikos$. 
A thread management and  CV (via sleep and wake-up) in our kernel heavily rely on this abstract queue library. 
The implementation of this abstract queue uses 
doubly linked lists; thus, each node in the lists is defined as 
\begin{lstlisting}[language = C, multicols=2]
// TDQ in C
struct TDQ_node{
uint head; uint tail; };
// TDQ in Coq
Inductive TDQueue :=
| TDQUndef | TDQValid (head tail: Z).   
\end{lstlisting}
To provide a  simple interface for this queue, 
we abstracts the queue state as $\coq$ lists as
\begin{lstlisting}[language = C]
Inductive AbQueue := | AbQUndef | AbQValid (l: list Z).
\end{lstlisting}
and local queue operations--
 \textit{enqueue} and \textit{dequeue}--are directly specified as the operations over the abstract lists as follows:
\begin{lstlisting}[language = C]
Function local_enqueue_spec (n i: Z) (adt: RData): option RData :=
  <@$\cdots$@>
  {abq: ZMap.set n (AbQValid (l ++ (i::nil))) (abq adt)}
  <@$\cdots$@>
\end{lstlisting}
Because those queues can be shared by multiple threads in the system, 
we associate each shared queue with a lock,
\begin{lstlisting}[language = C]
Function enqueue_atomic_spec (n i: Z) (adt: RData): option RData :=
  match acquire_lock_ABTCB_spec n adt with
  | Some adt0 => match local_enqueue_spec n i adt0 with
                             | Some adt1 => release_lock_ABTCB_spec n adt1
                             | _ => None
                             end
  | _ => None
  end.
\end{lstlisting}
and provide an atomic interface for the queue by using a \textit{log-lift} pattern in $\ccalname$.


\subsubsection{Thread Management}
Our thread-management module introduces a  thread-control block, 
that contains the status of each thread (\ie, ready, run, \etc)
as well as other information for the thread, including the information about its quotas, parent, and children.

A thread scheduler is associated with thread primitives; yield, sleep, and wake-up, which uses a kernel context switching and an abstract queue library. 
Each CPU has a private ready queue and a shared pending queue (for our condition variable).
This ready queue is private to each CPU, so, operations on the ready queue are only allowed by the owner of the queue.
On the other hand, other CPUs in the system can insert threads to the current CPUâ€™s pending queue. 
$\certikos$  also provides a set of sleeping queues, which are shared among all CPUs.
Each sleeping queue is associated with its condition variable (CV).

The \textit{yield} primitive works with three operations:
(1) when the pending queue is not empty, dequeue the head of its pending queue and add it to its ready queue;
(2) enqueue the running thread to the ready queue; and 
(3) dequeue the head of its ready queue to make it run.

The \textit{sleep} primitive has two steps;
(1) enqueue the running thread to the shared sleeping queue associated with the condition variable, which is an argument of the sleep primitive; and
(2) dequeue the head of its ready queue to make it run.

The \textit{wake-up} primitive contains two cases. 
The first is when the thread that wants to wake up is the thread on the current CPU. 
In this case, the primitive adds the thread to its ready queue because the primitive has permission to manipulate the ready queue directly.
However, when the thread that will be woken up is not in the current CPU the wake-up primitive adds the thread to the pending queue of the CPU
that the thread belongs to (\ie, notes that all CPUs can manipulate other CPUs' pending queues.) 

\jieung{I found that we cannot guarantee liveness of the thread in the pending queue when proc create is infinitely invoked because it does not
check the pending queue. But, it should be ok because our proc create is bounded with the maximum number of threads for each CPU.}
\jieung{Can I use the scheduling figure in certikos paper (Figure 9)? Even if I can use that one, I need to modify it by adding proc create.}

The thread spawn primitive also affects the scheduling.
When one thread wants to create its child, 
it first checks the quota that the child wants to have (\ie, it is the argument of the primitive)
and then creates the child by giving the new thread ID to it.
The thread then adds the newly created child at the end of the ready queue of the CPU and tells the CPU that the parent and child belong to it.
As seen in these scheduling procedures, we do not allow the thread migration from one CPU to another.

Like what our layers do for other shared primitives, $\certikos$ provides a layer interface ($\PBThrd$)
that contains atomic specifications for all scheduling-related primitives. 
The layer, $\PBThrd$, also merges all separate logs\footnote{Note that we divide the global log into multiple individual logs is associated with the corresponding shared objects from $\mmcsbootfull$. The division is for the simplicity of our layer building, but we have to merge them into the single global log for multithreaded linking. We mentioned this several times in Chapter~\ref{chapter:mcs-lock} and Chapter~\ref{chapter:linking}} 
into a single log to provide the top-most layer of our per-CPU modules (and thus the base layer for our multithreaded linking).
Figure~\ref{fig:chapter:certikos:proc-create-cpu-spec} and Figure~\ref{fig:chapter:certikos:thread-yield-cpu-spec} show the specification of the thread creation primitive and the specification of the thread yield primitive, respectively.

\begin{figure}
\lstinputlisting[language =C, deletekeywords={int}]{source_code/certikos/cpu_proc_create.v} 
\caption{Thread Spawn Primitive Specification in $\PBThrd$ (Written in $\coq$.)}
\label{fig:chapter:certikos:proc-create-cpu-spec}
\end{figure}


\begin{figure}
\lstinputlisting[language = C]{source_code/certikos/cpu_thread_yield.v} 
\caption{Thread Yield Primitive Specification in $\PBThrd$ (Written in $\coq$).}
\label{fig:chapter:certikos:thread-yield-cpu-spec} 
\end{figure}


\subsection{Connecting All per-CPU layers}
\label{chapter:certikos:subsec:connecting-all-per-cpu-layers}

We have introduced 60 layers for all those modules in our per-CPU layers, from a layer $\mmcsbootfull$  to a layer 
$\PBThrd$.
They can be linked together via the composition rules described in Section~\ref{chapter:ccal:subsec:linking},
thus forming a single contextual refinement theorem, which is defined in Theorem~\ref{theorem:chapter:certikos:per-cpu-contextual-refinement},
corresponding to the number (1) in Figure~\ref{fig:chapter:certikos:idea-of-dividing-certikos-with-formal-def}.
\begin{theorem}[Contextual Refinement for Per-CPU layers]
\label{theorem:chapter:certikos:per-cpu-contextual-refinement}
Let's assume that $cid$ is a valid CPU ID (in between 0 and 7), $\oracle_{\codeinmath{cpu}}$ and  $\oracle'_{\codeinmath{cpu}}$ are 
environmental contexts for $\mmcsbootfull$  and $\PBThrd$ layers, respectively, and $\codeinmath{CTXT}$ is an any-context program. Then, we can say that
\begin{center}
$\semwmachine{\mmcsbootfull[cid, \oracle_{\codeinmath{cpu}}]}{\certikos_{\codeinmath{cpu}} \oplus \codeinmath{Ctxt}}{\codeinmath{mach}_{\lasm}} \refines_{R_{\certikos_\codeinmath{cpu}}} \semwmachine{\PBThrd[cid, \oracle'_{\codeinmath{cpu}}]}{\codeinmath{Ctxt}}{{\codeinmath{mach}_{\lasm}}}$
\end{center}
when ${R_{\certikos_\codeinmath{cpu}}}$ is a refinement relation between those two layers (the refinement relation can be achieved by 
connecting all the refinement relations in between 60 per-CPU layers).
\end{theorem}

%Informally, this theorem states the following property:
%\begin{quote} 
%"any context program $\codeinmath{Ctxt}$ running on the $\PBThrd$ layer (the top-most layer interface of per-CPU layers) contextually refines the context program $\codeinmath{Ctxt}$ plus  the implementation of CPU local modules of $\certikos$, $\certikos_{\codeinmath{cpu}}$,
%running on $\mmcsbootfull$ (the bottom-most layer interface of per-thread layers) with LAsm".
%\end{quote}
%%

%This corresponds to the number (14) in Figure~\ref{fig:chapter:certikos:top-level-multicore-theorem}.

%
%\begin{lstlisting}[language=C]
%  Theorem Per_CPU_CertiKOS_correct:
%    forall (s: stencil) (CTXT: LAsm.module) kernel combined_program user_program
%           (builtin_idents_norepet_prf: CompCertBuiltins.BuiltinIdentsNorepet),
%      CertiKOS.per_cpu_certikos = OK kernel ->
%      make_program s (CTXT <@$\oplus$@> kernel) (MBoot.mboot <@$\oplus$@> L64) = OK combined_program ->
%      make_program s CTXT (PBThread.pbthread <@$\oplus$@> L64) = OK user_program ->
%      inhabited
%        (backward_simulation
%           (LAsm.semantics (lcfg_ops := LC (PBThread.pbthread <@$\oplus$@> L64)) user_program)
%           (LAsm.semantics (lcfg_ops := LC (MBoot.mboot <@$\oplus$@> L64)) combined_program)).
%\end{lstlisting}
