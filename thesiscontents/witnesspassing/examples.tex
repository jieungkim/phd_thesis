\section{Examples}
\label{chapter:witnesspassing:sec:examples}

\jieung{We hope to add more details later.}

We demonstrate the flexibility of \sysname{} by applying it to other leader-based distributed systems, Raft~\cite{raft}, two-phase commit~\cite{distsys}, and Lamport's distributed
lock~\cite{lamportclock}.

\subsection{Raft with Witness}
\label{chapter:witnesspassing:subsec:raft-with-witness}

Raft implements state machine replication similar to our multi-Paxos example and
has few commonalities. They use the same leader election scheme that elects
the leader based on the majority vote at each term and commits a new entry based on
successful write/accept to majority of nodes/acceptors. The key differences are, 
the Raft leader can make progress during network partition without confirming the 
commit of older requests, the Raft leader must have the longest log size, and 
the leader always checks and keeps consistency of all nodes during each write.
Compare to multi-Paxos, $\gldrfunction$ of Raft requires a simple additional
condition check for the log length before voting for the leader and this barely
changes the witness structure. However, Raft leader being able to make progress
during network partition makes $\gopfunction$ to be more complicated to
reason about. However, the linearization point of Raft is when a leader
successfully writes to the majority of nodes. Therefore, we can subdivide
$\gopfunction$ to try making state changes without confirming a successful
commit and if a node with missing log entry is found, switch to recovery mode
to retry an old request. The recovery mode can use the update function almost same 
as the update function in Section~\ref{chapter:witnesspassing:sec:multipaxos-with-witness}. It, however, 
requires a special case in the $g_{post}$ function to handle the recovery case, 
which does not update the leader's version number and the witness.  
as well as a quorum definition because it is between one approver and a leader.
\begin{center}
$
\begin{array}{lll}
u (st : \dsvalue) &:=&  \mbox{\textbf{match}} \ st \ \mbox{\textbf{with}}\\
&& \vert~(\_, (idx, cidx), dsval) \Rightarrow ((idx + 1, cidx + 1), dsval[idx + 1] := uf_{\dsvalue}\ dsval[idx])\\
&&\ \mbox{\textbf{end}}.\\
\end{array}
$
\end{center}
In either case, Raft protocol returns success only to a
linearizable write, which is an append to the last successful write. The
combination of the log length indicator changes the committed index and this
operation can be encoded into to post action of $\gopfunction$. Indeed,
because Raft employs a strong notion of a leader, the leader already contains
enough information about the global state of the system which is as much
information as our typical witness structure. This information can be given to
the template for the linearizability proof. 


\subsection{Two-phase Commit with a Membership Service}
\label{chapter:witnesspassing:subsec:two-phase-commit-with-a-membership-service}

Two-phase commit protocol itself typically does not include a membership
change scheme and we assume that there is a membership service which can makes one of the
node in the system to become the new transaction manager in case a transaction 
manager fails. 

A membership service can be a Zookeeper-like~\cite{zookeeper} system that is implemented on top
of multi-Paxos, but we assume a single front-end node for such system as the
approver that processes the leader election request. Thus, the leader election is 
as simple as sending the request to a single node and getting an acknowledgement back. 
However, data of the system is stored in the resource managers so the leader election and
commit operation happens in different set of nodes. Committing data requires sending
and receiving acknowledgements from all resource nodes that the transaction
touches.  Two-phase commit requires unanimous commit vote to commit a transaction, so the
logic in the requester side is simply making sure all requests are approved by
the resource nodes. 

\subsection{Lamport Lock with Witness}
\label{chapter:witnesspassing:subsec:lamport-lock-with-witness}

Lamport's lock can be thought of as a leader-based system where the owner of
the lock is the leader.
Each node maintains a logical clock that is incremented every time it sends or
receives a message and is included in sent messages.
Nodes also keep a priority queue of request messages ordered by timestamp.
A node initiates a request for the lock by adding the request to its own queue
and broadcasting it to every other node.
Upon receiving a request the node adds it to the queue and sends a timestamped
acknowledgement.
The lock is acquired when a node observes that its own request is at the front
of its queue and it has received a message from every other node with a later
timestamp.
At this point the node that owns the lock is guaranteed to have exclusive
access to the shared state and is free to modify it.
To release the lock it removes its request from its queue and broadcasts a
message telling other nodes to do the same.
To model it, $\dsvalue$ contains a bit ($lock$) to indicate whether the node holds the lock or not.
$\ldrfunction$ acquires the lock and $\opfunction$ with the specific update function 
releases the lock as follows:
\begin{center}
$
\begin{array}{lll}
u (st : \dsvalue)&:=& \mbox{\textbf{match}} \ st \ \mbox{\textbf{with}}\\
&&\vert~(\_, idx, dsval) \Rightarrow (idx, dsval.lock := false)\\
 && \mbox{\textbf{end}}.\\
\end{array}
$
\end{center}
