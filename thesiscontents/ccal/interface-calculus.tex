\section{Formal Definitions of CCAL}
\label{chapter:ccal:sec:interface-calculus}

Followed by the previous section, 
this section defines formal definitions of our certified concurrent abstraction layers,
including the concurrent layer interface and the layer calculus, 
 with more details. 


\subsection{Multiprocessor Machine Model}
\label{boot-total}

We first define a machine model for the full participants ($D$), which is a multiprocessor $\intelmachine$ machine model. 
This machine model is a part of TCB in our framework, but 
it  is a conventional model of sequentially consistent program
execution, and allows arbitrary interleaving
and shared-memory
accesses. 


\subsubsection{Machine State} 
The state of  $\intelmachine$ machine model is denoted as a tuple ``$\state := (c, f_\regs, m, a, l)$.''
The definition includes
the current running CPU ID $c$ and 
all CPUs' private states $f_\regs$, which is a partial map from CPU ID to  private state $\regs$.
I have shared states,
$m$ (a shared memory) and $a$ (an abstract state)
and $l$ (a global log).
The abstract state $a$  is generally used in our layered approach to
summarize and abstract in-memory data structures from lower layers.
It replaces in-memory data structures into abstract representations 
by hiding low-level details in the memory (\eg. buffer overflow). 
It also affects program execution when making primitive calls. 

The global log $l$ is a list of observable events 
that contains all shared operations in the system. 


Events generated by different CPUs are
interleaved in the log, following the actual chronological order of events.
Therefore, all shared state, including the shared memory
and shared abstract state, can be reconstructed
from the log $l$ alone by using \emph{replay function}.
The global log is also used as an index to query the environment context
$\oracle$, which returns the next event generated by environment CPUs.





We define a machine model for the full participants ($D$), which is $\intelmachine$. 
is a fairly conventional model of sequentially consistent program
execution, which allows arbitrary interleaving
and shared-memory
accesses. 


It would be difficult to directly verify programs running 
over $\Mach_{\boot}$, as it would require
simultaneously considering
all possible interleaving among all CPUs.
Instead, we show in Sec.~\ref{boot-partial} how to enable local reasoning
by introducing the concurrent layer interface.
The concurrent layer $\PLayer{L}{A}{\oracle}$ represents program execution 
on a parameterized CPU set $A$,
and specifies a set of \emph{environment contexts} $\oracle$
that are ``acceptable'' by this layer. The environment context $\oracle$
models both a logical \emph{hardware scheduler} $\strat{hs}$
that picks a particular interleaving, as well as all 
\emph{observable events} produced by CPUs that are not present in 
the parameterized CPU set $A$.

When taking  a single CPU $c$ as the parameter,
all  transitions
fall into two categories:
local execution on CPU $c$
and environmental execution  interacting with $\oracle$.
We refer to such a layer $\PLayer{L}{c}{\oracle}$
as a ``CPU-local layer'' because the programs running on 
it can be verified in a sequential style.

Finally, we describe in Sec.~\ref{boot-linking} the layer calculus used to build and compose concurrent layer interfaces.
We show how we
prove a multicore linking theorem, stating that there is a refinement connecting $\Mach_{\boot}$ to
a layer that runs all of the individual CPU-local layers 
in parallel.
This means that we can reason about a program over the CPU-local layer
using usual verification techniques for sequential programs,
then conclude the correctness of the whole system over the
multiprocessor machine model.




\subsubsection{Transition Relation.} The machine $\Mach_\boot$
has two types of transitions that are arbitrarily and nondeterministically 
interleaved: 
program transitions and hardware scheduling.

\emph{Program transitions} are one of three possible types:
instruction executions, private primitive calls, and shared primitive calls.
The first two types are ``silent'', in that they do not
generate events.
Shared primitives, on the other hand, provide the only means 
for accessing and appending events to the global log.
The transitions for instructions only change $\regs$, $pm$, and $m$, and are defined as standard operational semantics
for C or x86-assembly, similar to (and in fact based on) the
operational semantics used in
CompCert~\cite{leroy09}. 
Primitive calls are specific to our style of verification: they
directly specify the semantics of function $f$ from underlying layers as a relation $\spec_f$
defined in Coq. This relation specifies how the state is updated after $f$
is called with the given arguments and what the return value is.

\subsubsection{Hardware scheduling} 
transitions  change the
current CPU \allid{} $c$ to some  \allid{} $c'$. 
They also record this 
change of CPU \allid{} as an observable event $(c \switch c')$ in the global log.
These hardware scheduling
and can be arbitrarily interleaved with
program transitions.
In other words, at any step,
$\Mach_{\boot}$ can take either a program transition staying
on the current CPU,
or a hardware scheduling to another CPU.
The \emph{behavior} of a client program $P$ over this multicore machine (denoted as $\sem{\Mach_\boot}{P}$) is  a set of global logs generated by executing $P$ via these two kinds of transitions.

\subsubsection{Memory Model.}
In the multiprocessor setting,
one major challenge is to model shared-memory accesses
in a way that enables local reasoning.
On one hand, we have to expose shared-memory changes
at the log level, making their effects visible to the whole system.
On the other hand,  we should only expose the resulting memory
content and hide the concrete manipulation details.
Furthermore, we also rely on the memory model
to detect and forbid dangerous accesses like data races.
To address these issues, we introduce
a  ``push/pull'' memory model for the shared memory $m$
(the private memory is separately handled in $\regs$),
which encapsulates the shared memory operations
into $\push/\pull$ events and can detect data races.

In this model, each shared memory location $b$ is associated
with an ownership status in the abstract state $a$, which can only be manipulated by 
two shared primitives called $\cpull$ and $\cpush$. The $\cpull$ operation modifies the ownership from ``free'' to  ``owned by $c$'', after which shared memory accesses
can be performed by CPU $c$. The $\cpush$ operation frees the ownership and records its memory updates in the log.
Figure~\ref{fig:exp:lpull} shows  the specification $\spec_\pull'$
, where ``$r\ \set{i:v}$''
means updating the record $r$ at field $i$ 
with value $v$.


\begin{figure}[t]
\lstinputlisting [language = Caml] {source_code/ccal/lpull.v}
\caption{Pseudocode of the $\pull$ specification of $\Mach_{\boot}$  in Coq.}
\label{fig:exp:lpull}
\end{figure}


\subsection{Concurrent Layer Interface}
\label{boot-partial}
We now zoom in on the execution of a subset of CPUs $A$,
introducing the \emph{concurrent layer interface} $\PLayer{L}{A}{\oracle}$
defined as a tuple
$(\Layer, \Rely, \Guard)$.
The machine based on this concurrent interface is ``open'' in the sense that it is eligible to  capture  a subset of the
CPUs and then be composed with any acceptable execution of the rest of CPUs. 
The domain of the private state map $f_\regs$ is also this captured (or focused) subset.
The interface $\PLayer{L}{A}{\oracle}$  equips this open machine with a 
collection of primitives that  are defined in $\Layer $ and can be invoked  at this level,
the rely condition $\Rely$  that
specifies a set of acceptable environment 
contexts, and
the guarantee condition $\Guard$
 that the log $l$ should hold.
The instruction transitions are defined as before,
but all hardware scheduling is replaced by queries to
the environment context.

\para{Environment Context.} $\oracle$
is a partial function from a CPU ID to its \emph{strategy} $\strat{}$. A strategy is an automata
that generates events in response to given logs. 
When focusing on a CPU set $A$, all the observable behaviors of the hardware scheduling
and the program transitions of other CPUs
can be specified as a union of strategies (i.e.\, $\oracle$).
This union serves
as a specification for other CPUs and the hardware scheduler.
Thus,
whenever there is a potential interleaving,
the machine can \emph{query} $\oracle$ about the events from other CPUs (and the  scheduler).

These environmental events cannot influence the behaviors
of  instructions and private primitive calls. This also applies to
shared memory read/write, because the push/pull memory model encapsulates
other CPUs' effects over the shared memory into $\push$/$\pull$ events.
Thus, during the execution of instructions
and private primitives, it is unnecessary to query $\oracle$, and the appended environmental events will be received by the next
so-called \emph{query point}, that is, the point just
before executing shared primitives.
The intuition of this query point is that
the current CPU only needs to be informed of other CPUs' non-local changes
just before invoking its own shared primitives.

To be more specific, at each query point, the machine repeatedly
queries $\oracle$.
Each query takes the current log $l$
as the argument and returns an event (i.e.\, ``$\oracle(c', l)$'') from a CPU $c'$ not in
$A$. That event is then appended to $l$, and this querying continues
until there is a hardware transition event back to $A$ (assuming the hardware scheduler is fair).
In the following, we write
``$\Query{\oracle}{l}{A}$'' to mean this entire process of extending $l$ with
multiple events from other CPUs. 

In this way, we are able to define the semantics of instructions
and private primitives in a sequential style 
with considering concurrency
and interleavings  just only before 
shared primitive calls by querying its environmental context.
This makes concurrent program verification far more scalable.

\para{Rely and Guarantee Conditions.}
The $\Rely$ and $\Guard$ of the layer interface specify the validity of the environment context and the invariant of the log (containing the locally-generated events).
After each step of threads in $A$ over interface $L[A]$, 
the resulting log $l$  must satisfy the guarantee condition $L[A].\Guard$, i.e.\, $l \in L[A].\Guard(c)$ if $c$ is the current CPU ID indicated by $l$. 
To prove that
 guarantee conditions always hold,  we not only need to validate the events generated locally
but also need to rely on the validity of the environment context. The rely condition $L[A].\Rely$ specifies a set of
valid environment contexts, which take valid input logs
and return a valid list of events.

\para{CPU-Local Layer Interface.}
When focusing on a single CPU $c$, $\PLayer{L}{c}{\oracle}$ is called 
a CPU-local layer interface.
Its machine state is $(\regs, m, a, l)$,
where $\regs$ is the private state of the CPU $c$
and $m$ is just a \emph{local copy} of the shared memory.

\begin{figure}[t]
\lstinputlisting [language = Caml] {source_code/ccal/push.v}
\caption{Pseudocode of $\push/\pull$ specifications of $L[c]$ in Coq.}
\label{fig:exp:pull}
\end{figure}


This $m$ can only be accessed locally by $c$. The primitives $\cpush/\cpull$ of
$L[c]$  ``deliver'' the effects of shared memory operations.
In detail, the memory ($m$) can be treated locally
because the current CPU $c$ can only observe 
the context events instead of other CPUs'  concrete memory
operations. The memory contents are ``delivered''  by the  shared primitives
between $c$ and other CPUs.
On the other hand, 
shared memory updates from other CPUs are ``retrieved'' by the $\cpush/\cpull$ primitive, 
which  queries $\oracle$, updated the log
with the query result, and generates the $\cpull$ event.
Figure~\ref{fig:exp:pull} shows their specifications, which depend on
a replay function  $\replay_{\codeinmath{shared}}$ to
reconstruct the shared memory value $v$ for some location $b$ and check the well-formedness (i.e.\, no data race occurs)
of the resulting log.

Since $\spec_\pull$ is parameterized with $\oracle$, it can also be viewed as the following special strategy  with private state updates:%
\begin{center}
\begin{tikzpicture}[->,>={stealth[black]}, auto,  node distance=3cm,draw]
\begin{scope}[every node/.style={font=\sffamily\small}]
    \node (A) at (0,0) {};
    \node (B) [node_w] at (0.5,0) {};
    \node (C) [node_db] at (4.5,0) {};
\end{scope}

\begin{scope}[every node/.style={font=\sffamily\small},
every edge/.style={draw, thick}]
    \path [->] (A) edge (B);
    \path [->] (B) edge node[above] {$?\oracle, !c.\pull(b), \return (v, \codeinmath{own}\ c)$} node[below]{$\set{m.b: v}$} (C);
\end{scope}
\end{tikzpicture}
\end{center}
The layer machine enters the \emph{critical state} after calling $\codeinmath{pull}$ by holding the ownership
of a shared location.
With the replay function ($\replay_{\codeinmath{shared}}$) and the given log, 
we can derive the value that is mapped with $b$ is $v$, 
and the $\pull$ primitive
stores  $v$  into the memory, updates the clean status,
and finally generates a $\pull$ event.
The memory state $m$ is updated using the contents delivered
by the $\cpull$ operation
and  the clean status is also marked to be $\codeinmath{dirty}(c)$,
meaning that it is now only safe for CPU $c$ to access.
Thus, this memory location will be treated as
a \emph{local}
copy of the shared state being accessed by $c$.
After the $c.\pull$ operation transfers the contents,
all subsequent shared accesses are performed over this 
copy as if it were private to CPU $c$.
It exits the critical state by invoking $\codeinmath{push}$ to free the ownership.
This $\cpush$ primitive also delivers the updated memory contents back 
by appending the $\cpush$ event that contains the value $v$ into the log.




\subsection{Layer Linking}
\label{chapter:ccal:subsec:linking}

To build and compose concurrent layers, 
our framework also extends the layered calculus in $\calname$~\cite{deepspec}
Our definition borrows the notations
from $\calname$
\begin{itemize}
\item $\varnothing$ : An  empty program module
\item $\oplus$ : the union of two modules (or two layers' primitive collections)
\item $(i\mapsto \cdot)$ : a singleton map with a pointer or location $i$ as its domain.
\end{itemize}

Two rules, \textsc{Empty} and \textsc{Run}, works as bases of our layered calculus , which 
provide identity relation and a relation for one primitive. 
 \begin{mathpar}
\inferrule*[Right=Empty]{ }{\ltyp{\PLayer{L}{A}{\oracle}}{\id}{\varnothing}{\PLayer{L}{A}{\oracle}}}
\and
\inferrule*[Right=Fun]{\ssem{\kappa}{\PLayer{L}{c}{\oracle}} \le_R \spec}{\ltyp{\PLayer{L}{c}{\oracle}}{\id}{i \mapsto \kappa}{i \mapsto \spec}}
\end{mathpar}

The vertical composition rule (\textsc{Vcomp}) allows us
to verify the modules $M$ and $N$ (where $N$ may depend on $M$) 
in two separate steps.
    \begin{mathpar}
\inferrule*[Right=Vcomp]{ \ltyp{\PLayer{L_1}{A}{\oracle_1}}{R}{M}{\PLayer{L_2}{A}{\oracle_2}} \\
 \ltyp{\PLayer{L_2}{A}{\oracle_2}}{S}{N}{\PLayer{L_3}{A}{\oracle_3}}}{\ltyp{\PLayer{L_1}{A}{\oracle_1}}{R \circ S}{M \oplus N}{\PLayer{L_3}{A}{\oracle_3}}}
\end{mathpar}

The horizontal composition rule
(\textsc{Hcomp}) enables local reasoning for independent
modules $M$ and $N$ belonging to the same level. These two composition rules can only compose layers over the same  CPU set.
 \begin{mathpar}
\inferrule*[Right=Hcomp]{
\ltyp{\PLayer{L}{A}{\oracle}}{R}{M}{\PLayer{L_1}{A}{\oracle'}} \\ \ltyp{\PLayer{L}{A}{\oracle}}{R}{N}{\PLayer{L_2}{A}{\oracle'}} \\\\
L'[A].\Layer=L_1[A].\Layer\oplus \L_2[A].\Layer \\
L'[A].\Rely=L_1[A].\Rely = \L_2[A].\Rely \\
L'[A].\Guard=L_1[A].\Guard = \L_2[A].\Guard
}{\ltyp{\PLayer{L}{A}{\oracle}}{R}{M \oplus N}{\PLayer{L'}{A}{\oracle'}}}
\end{mathpar}

Besides them, 
the parallel compositions rule (between $L[A]$ and $L[B]$) 
also can be defined in a logical level. 
It is, however, not well supported in our framework since we do not provide
any canonical forms to support the parallel composition 
in our implementation.
Another part of this dissertation (Chapter~\ref{chapter:linking}) addresses 
the parallel composition. 


