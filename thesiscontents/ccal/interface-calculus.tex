\section{Layer Interfaces and Calculus}
\label{chapter:ccal:sec:interface-calculus}

Followed by the previous section, 
we present formal definitions of our $\ccalname$.
Besides those definitions, 
we also provide a correctness condition in building layers, which is 
a contextual refinement theorem between two adjacent $\ccalname$,
as well as an extension of the theorem by linking multiple layers together using layer calculus.


\subsection{Multicore Machine Model}
\label{chapter:ccal:subsec:multicore-machine-model}


We first define a machine model for full participants ($D$ and  a scheduler), which is a multicore machine model, 
to provide a better understanding of our machine model for local layers.
This multicore machine model is part of TCB in our framework, but it can be instantiated as a conventional model (such as $\intelmachine$--see Chapter~\ref{chapter:linking} for  details) with enforcing a sequentially consistent program
execution and allowing an arbitrary interleaving
and shared memory accesses. 


\subsubsection{Machine State} 
A state of a  multicore machine model is denoted as a tuple ``$\state := (c, f_{\regs}, m, a, l)$"
consisting of
 a current running CPU ID $c$,
all CPUs' private states $f_\regs$ (a partial map from CPU ID to  private state $\regs$)
 a shared memory $m$,
 a shared abstract state $a$,
and a global log $l$ (a single list with events).
An abstract state $a$ 
summarizes and abstracts in-memory low-level data structures from lower layers. 
This abstract representation replaces
in-memory data structures by hiding low-level details in the memory (such as padding, aligning, and overflow)
but it is not only an abstraction of parts of memory but also a state that affects program execution when making primitive calls. 
A global log $l$ is a list of observable events--a history of all shared operations--in the system.
Those events are
interleaved in the log with the chronological order of events for all operations that are visible to other components in the system. 
Therefore, 
two shared parts in our state definition (the shared memory $m$  
and the shared abstract state $a$)
can be reconstructed
from the log $l$ alone  using a \emph{replay function} $\replay$,
the key idea of which is briefly discussed in Section~\ref{chapter:ccal:subsec:concurrent-layer-with-environment}.


\subsubsection{Memory Model}

Modeling shared-memory accesses is
a significant challenge in concurrent program verification;
it should enable local reasoning while reflecting the manipulations of other participants.
On the other hand, 
with the proper semantics of sequential consistency execution,
our model needs to enforce that the memory always memorizes all changes in the global log to make them visible to the whole system.
Besides,   
each CPU does not need to know in detail how other CPUs 
change the memory, but it has to keep track of the result of those changes.
We should  only expose the resulting memory 
content and hide the concrete manipulation details.
Further, we also rely on the memory model
to detect and forbid dangerous accesses such as data races.


\begin{figure}
\includegraphics[width=\textwidth]{figs/ccal/pushpullsharedmemory}
\caption{Push/Pull Memory Model and Shared Memory.}
\label{fig:chapter:ccal:push-pull-memory-model-and-shared-memory}
\end{figure}

We introduce a ``push/pull'' memory model for a shared memory $m$ to address all the issues we mentioned and 
Figure~\ref{fig:chapter:ccal:push-pull-memory-model-and-shared-memory} shows 
how this model works as well as how push/pull events correspond to changes in the associated memory block and an ownership field of a shared abstract state.
The memory is a map from a block identifier to its value,
and the ownership field of our shared abstract state is also a map from a block identifier to the ownership flag.
Only two shared primitives, $\pull$ and $\push$,
provide a way to manipulate them.
In this example, CPU 1 tries to access a memory block $b$ by calling a primitive $\pull$. 
The $\pull$ operation modifies the ownership from \code{free} to \code{own} $1$. 
Then, other CPUs cannot change the ownership flag of the block until CPU $1$ changes it back to \code{free} 
by a $\push$ operation.
The $\push$ operation also updates the shared memory block $b$ (from $v$ to $v'$), which is the result of the operations for the memory block $b$ performed by  CPU $1$  between $\pull$ and $\push$ calls. 
Those two shared operations also generate $1.\push(b)(v)$ ($v$ is a memory snapshot when CPU $1$ starts its shared memory manipulation) 
and  $1.\pull(b)(v')$ ($v'$ is a result value of shared memory manipulation by CPU $1$ in the example), respectively.
By doing that, those two $\push$ and $\pull$ encapsulate the shared memory operations
into $\push/\pull$ events and they can detect data races.

\subsubsection{Transitions}

There are two kinds of transitions--program transitions and scheduler transitions--in our multicore machine,
which are arbitrarily and nondeterministically interleaved together.


\textbf{Program transitions} are one of two possible types:
a private execution, and a shared execution.
The first only changes private states associated with the CPU ID and is not visible to other CPUs;
thus we are safe to treat it as a ``silent'' step without generating any events. 
The other, however, 
accesses shared states (a shared memory and/or a shared abstract state),
so executing it is  the only way for our machine model  to access and append events to the global log.
Figure~\ref{fig:chapter:ccal:push-pull-memory-model-and-shared-memory} is one example of shared operations.

When we model our multicore machine as a $\intelmachine$ model (with multiple cores),
those program transition relations can be connected with the relations 
in the machine model of $\compcertx$~\cite{deepspec},
which is a variant of $\compcert$~\cite{leroy09}
by adding primitive calls which are specific to our style of verification.
Primitive calls directly specify the semantics of function $f$ from underlying layers defined in $\coq$. 
This relation determines the state update after the $f$ call (with the given arguments) and what the return value is.
We show how we efficiently connect program transition relations in 
$\intelmachine$ model with transition rules in $\compcertx$ in Chapter~\ref{chapter:linking}.

\textbf{Scheduler transitions} are moves in our multicore machine model performed by one logical component, a hardware scheduler.
 These transitions determine  the CPU that will perform its program transition 
in the next turn and 
an instance of all possible nondeterministic scheduler transitions provide 
the strategy   ($\strat{sched}$) for this logical component, 
which will be a part of an environmental context for other machine models. 
In detail, when the current CPU ID $c$ changes
the control to CPU ID $c'$,
the hardware scheduler works as a
middleman between 
two CPUs and records the scheduling event 
in the global log ($c \switch c'$).
These hardware scheduling events
can be arbitrarily interleaved with
program transitions.
In other words, at any step,
an $\intelmachine$ machine can take either a program transition staying
on the current CPU
or a hardware scheduling to another CPU.
The \emph{behavior} of a client program \code{P} over this multicore machine is a set of global logs generated by executing \code{P} via these two kinds of transitions.
It is also a way for us to hide nondeterminism in the higher layers
by picking  
one fixed hardware-scheduling strategy for the verification process.

\subsection{Partial  Machine Model and Concurrent Layer Interfaces}
\label{chapter:ccal:subsec:concurrent-layer-interface}


A state definition ``$\state := (c, f_{\regs}, m, a, l)$'' and a memory model in our partial machine model are
same as those of a multicore machine model, 
except the domain of its private state ($f_{\regs}$) is only for the focused set $A$ instead of the full CPU set $D$.
The partial machine model, however, requires one more transition rule to handle environmental steps for the participants that are not in the focused set.
When focusing on a partial machine working with a subset of CPUs $A$,
the execution of the machine needs to handle 
the steps for its environments ($D - A$ plus the hardware scheduler) with a \textit{valid} environmental context ($\oracle$);
thus, we want to  achieve a way to semantically verify the part for $A$ of the whole program locally
by making  assumptions on the rest of the programs (as discussed in Section~\ref{chapter:ccal:subsec:concurrent-layer-with-environment}).

Those assumptions  also need to be able to be guaranteed by executions of the other part of the entire system for 
the compositionality of multiple partial machines.
For example, the assumption of transitions with $L[A, \oracle_{A}]$ should be consistent with the invariants guaranteed by transitions of $L[(D - A), \oracle_{(D-A)}]$ and vice versa, when $D$ implies a set that contains whole CPUs.
In this sense, our layer can be defined as
$L[A, \oracle] := (\Layer, \Rely,\Guard)$ when $\Layer$ is a set of primitives in the layer $L$,
$\Rely : log \rightarrow Prop$ contains  ``rely'' invariants that provide assumptions about an environmental movement,
and $\Guard: log \rightarrow Prop$ represents ``guarantee'' invariants that local movements of $L[A, \oracle]$ need to satisfy.

\emph{Environmental transitions} then can be simply defined as 
an environmental context ($\oracle$) query when participants outside  the focused set perform transitions,
thus returning a particular sequence of events for one specific run of the concurrent program 
that satisfies the $\Rely$ condition.
Environmental transitions also include  transitions performed by 
a hardware scheduler, which is constructed by 
a single instance ($\strat{sched}$) of possible strategies of the scheduler.
All transitions are also deterministic thanks to our environmental context.
The nondeterminism of our multicore machine model is hidden in building strategies by
program executions with the machine (\eg, events generated by the hardware scheduler).

\subsection{Local Machine Model and Local Layer Interfaces}
\label{chapter:ccal:subsec:local-layer-interface}

With the singleton focused set, $\set{c}$, 
simplification is possible.
The state for the singleton set is a tuple ``$(\regs, m, a, l)$''
where $\regs$ is a private state of the focused CPU $c$
and $m$ is just a \emph{local copy} of a shared memory.


This $m$ can only be accessed locally by $c$ with  $\push$ and $\pull$ primitives discussed in our multicore memory model. 
Shared  memory updates by $c$ in this local copy, however, 
keep the consistency with 
 updates by $c$ at the shared memory model in
Section~\ref{chapter:ccal:subsec:multicore-machine-model}.


\begin{figure}
\includegraphics[width=\textwidth]{figs/ccal/pushpullsharedmemorylocal}
\caption{Push/Pull Memory Model and Local Memory.}
\label{fig:chapter:ccal:push-pull-memory-model-and-local-memory}
\end{figure}

Figure~\ref{fig:chapter:ccal:push-pull-memory-model-and-local-memory} shows how
all local memory updates keep  consistency with the corresponding shared memory updates. 
In the example, the model has the same sequence of updates with the example 
in Figure~\ref{fig:chapter:ccal:push-pull-memory-model-and-shared-memory}.
The local memory update by  CPU $1$ needs to be followed by the corresponding $\pull$ event, 
so the pull event, $1.pull(b)(v)$, tells the local machine that the current memory value is 
$v$ for the block $b$ by replaying the global log up to that point ($l$)
and updates the block in the local copy $m$ with the value ($b \rightarrow v$). 
After that, CPU $1$ operates on the memory based on the value $v$ until it pushes the change (as well as frees the ownership for the block).
The value $v$ for the block in here is the same value in the global shared memory accessed by multiple CPUs. 
All shared memory (and all abstract state) updates are recorded in the global log; thus,
updating the local copy of block $b$ by replaying the log $l$ gives the same status for the shared memory block $b$ (in multicore machine mode) at the moment
(\ie, notes that the local copy of the memory breaks the consistency when it does not have its ownership for the memory block). 

Similarly,  in between $(1.\pull(b)(v))$ and $(1.\push(b)(v'))$, 
CPU $1$ is the only  CPU that can modify the contents in the block $b$
because other CPUs cannot have ownership for the block.
Therefore, the local update for the memory block $b$ in the log $l'$ 
is consistent with the operations for the block $b$ of the shared memory (in our multicore machine model).


The machine model for this singleton focused set has significant similarity to the machine model in $\compcertx$. 
We, therefore, can easily connect this machine model with the $\compcertx$ compiler, 
so our local layer interfaces can freely use the correctness guarantee of the compiler for C. 
Due to this advantage of local machine model, local layer interfaces are the only place where we build layers that facilitate multiple libraries and generic proofs of the previous work for sequential programs~\cite{deepspec}. 
When building private primitives that only update the private state of $c$, 
layer building is same as the layer constructions in $\calname$.
Building layers for shared primitives, however, is slightly different and usually can be divided into 
two patterns. 
Similar to $\calname$, layers built by two patterns are connected via contextual refinement between layers. 

\subsubsection{Fun-lift}
A fun-lift pattern usually abstracts shared physical memory into shared abstract states,
which is similar to  abstraction layer building for sequential programs.
This pattern does not change any potential interleavings, 
so the environmental context in a layer $L_{low}[c, \oracle_{low}]$ remains 
the same as in a higher layer $L_{high}[c, \oracle_{high}]$ ($\oracle_{low} = \oracle_{high}$), and the 
relation for two environmental contexts is defined as an identity relation 
for a contextual refinement property between two layers (see Definition~\ref{def:chapter:ccal:contextual-refinement}).
This fun-lift pattern, however, possibly breaks an atomicity because the behavior of $L_{high}$ depends on other concurrent programs’ behavior in the middle of  primitive executions. 

\subsubsection{Log-lift}

Once all the low-level details are abstracted away for the shared primitive via using a \textit{fun-lift} pattern, we use a \textit{log-lift} pattern to lift non-atomic behaviors on shared abstract states to atomic behaviors--which only generate a single atomic event as the strategy for the associated primitive call-- on the states. 
Proving a contextual refinement with a \textit{log-lift} pattern requires reductions on the log, 
which includes merging and shuffling events to show 
that a consecutive list of  events in an underlay, $L_{low}[c, \oracle_{low}]$ , corresponds to a single new event in an overlay, 
$L_{high}[c, \oracle_{high}]$.  Changes of events generated by local movements
imply that this refinement also needs to relate two different environmental movements specified by $\oracle_{low}$ and $\oracle_{high}$.
Section~\ref{chapter:ccal:subsec:local-layer-with-environmental-context-and-local-layer-linking} shows the relation,
which is $\forall\ (j \in \codeinmath{EnvSet})\ 
(\strat{j} \in \oracle(c)) .\ \exists \stratp{j} . \ \stratp{j} \in \oracle'(c) \wedge R_{\strat{}}(\stratp{j}, \strat{j})$ when 
$\codeinmath{EnvSet} = ((D - \{ c\}) \cup \{ sched\})$


\subsubsection{Contextual Refinement}

The contextual refinement property for the local layer interfaces also requires modification to cover those patterns uniformly. 
A contextual refinement property in this case similarly uses a forward simulation technique as shown in  
Section~\ref{chapter:ccal:sec:cal}.
On the other hand, one challenge in defining a refinement property here is that the relation for it is not only related to layer definitions but also to different environmental contexts over two layers. 
\begin{figure}
\begin{center}
\includegraphics[width=0.6\textwidth]{figs/ccal/locallayerrefinement}
\end{center}
\caption{Simulation Relation Between Two Local Layers.}
\label{fig:chapter:ccal:refinement-between-two-layers}
\end{figure}
Figure~\ref{fig:chapter:ccal:refinement-between-two-layers} shows the key simulation relation for 
a contextual refinement property with two concurrent layers, which is stated  in Definition~\ref{def:chapter:ccal:contextual-refinement}.
\begin{definition}[Contextual Refinement]
\label{def:chapter:ccal:contextual-refinement}
$$
\forall \ i\  \codeinmath{Ctxt} \ \oracle \ \oracle .\ \sem{L_{low}[i, \oracle]}{\codeinmath{M}_{high} \oplus \codeinmath{Ctxt}} \refines_R \sem{L_{high}[i, \oracle']}{\codeinmath{Ctxt}}
$$
\end{definition}
This contextual refinement property is informally expressed as follows:
\begin{quote}
``For every thread $i$,  code modules $\codeinmath{M}_{high}$ and  $\codeinmath{Ctxt}$,  and environmental contexts
  $\oracle$ and $\oracle'$ (for thread $i$), we say an underlay system   $\sem{L_{low}[i, \oracle]}{\codeinmath{M}_{high} \oplus \codeinmath{Ctxt}}$
     \textit{contextually
 refines} an overlay system $ \sem{L_{high}[i, \oracle']}{\codeinmath{Ctxt}}$ when there always exist 
 multiple  transition steps in the underlay system for every  single  transition step
  in the overlay system that can be associated with a relation $R$,
which also contains a relation between strategies ($R_{\strat{}}$)
 of two layer systems that satisfies the property:
$\forall\ (j \in \codeinmath{EnvSet})\ 
(\strat{j} \in \oracle(i)) .\ \exists \stratp{j} . \ \stratp{j} \in \oracle'(i) \wedge R_{\strat{}}(\stratp{j}, \strat{j})$ when 
$\codeinmath{EnvSet} = ((D - \{ i\}) \cup \{ sched\})$.''
\end{quote}

{\noindent}We also state this contextual refinement with simpler notation, 
``$\ltyp{\PLayer{L_{low}}{i}{\oracle}}{R}{\codeinmath{M}_{\codeinmath{high}}}{\PLayer{L_{high}}{i}{\oracle'}}$.''



\subsection{Layer Linking}
\label{chapter:ccal:subsec:linking}


To build and compose concurrent layers, 
our framework also extends a layered calculus in $\calname$~\cite{deepspec} by 
borrowing the following notations 
from $\calname$:
\begin{itemize}
\item $\varnothing$ : An  empty program module
\item $\modulecomposekwd$ : a union of two modules (or two layers' primitive collections)
\item $\relcomposekwd$ : a composition of two refinement relations (between layers)
\end{itemize}

An empty rule works as the basis of our layered calculus.
 \begin{mathpar}
\inferrule*[Right=Empty]{ }{\ltyp{\PLayer{L}{c}{\oracle}}{\id}{\varnothing}{\PLayer{L}{c}{\oracle}}}
\end{mathpar}

A vertical composition rule allows us
to verify two modules $M$ and $N$ (where $N$ may depend on $M$) 
in two separate steps and allows us to build layer stacks by combining them.
    \begin{mathpar}
\inferrule*[Right=Vcomp]{ \ltyp{\PLayer{L_1}{c}{ \oracle_1}}{R}{M}{\PLayer{L_2}{c}{ \oracle_2}} \\
 \ltyp{\PLayer{L_2}{c}{\oracle_2}}{S}{N}{\PLayer{L_3}{c}{ \oracle_3}}}{\ltyp{\PLayer{L_1}{c}{\oracle_1}}{\relcompose{R}{S}}{\modulecompose{M}{N}}{\PLayer{L_3}{c}{\oracle_3}}}
\end{mathpar}

A horizontal composition rule enables local reasonings for independent
modules $M$ and $N$ belonging to the same level. Note that rely and guarantee invariants and the associated environmental context in $L_1$ and $L_2$ should 
be same for this horizontal composition.
 \begin{mathpar}
\inferrule*[Right=Hcomp]{
\ltyp{\PLayer{L}{c, \oracle}{}}{R}{M}{\PLayer{L_1}{c, \oracle_1}{}} \\ \ltyp{\PLayer{L}{c, \oracle}{}}{R}{N}{\PLayer{L_2}{c, \oracle_2}{}} \\\\
L'[c, \oracle'].\Layer=L_1[c, \oracle_1].\Layer\oplus L_2[c, \oracle_2].\Layer \\
L'[c, \oracle'].\Rely=L_1[c, \oracle_1].\Rely = L_2[c, \oracle_2].\Rely \\
L'[c, \oracle'].\Guard=L_1[Ac, \oracle_1].\Guard = L_2[c, \oracle_2].\Guard \\
\oracle = \oracle_1 = \oracle_2
}{\ltyp{\PLayer{L}{c,\oracle}{}}{R}{M \oplus N}{\PLayer{L'}{c, \oracle'}{}}}
\end{mathpar}



Besides these, 
a concurrent linking rule (\ie, $L[C, \oracle_C] = \parallelcompose{L[A, \oracle_A]}{L[B, \oracle_B]}$, when $C = A \cup B$ and $\oracle_C = \oracle_A \cap \oracle_B$)  
also can be defined in a conceptual level for any layers.
It is, however, not well supported in our framework; we do not provide
a canonical form to support the concurrent linking
in our implementation.
Chapter~\ref{chapter:linking} in this dissertation addresses this concurrent linking rule with more details.






